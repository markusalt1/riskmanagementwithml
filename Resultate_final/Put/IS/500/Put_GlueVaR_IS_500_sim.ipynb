{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Results were produced in stints. This is the number of the last stint.\r\n",
    "run = 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from scipy import stats\r\n",
    "from scipy import optimize\r\n",
    "import joblib\r\n",
    "\r\n",
    "#folder for saving results\r\n",
    "filepath = \".../Resultate_final/Put/IS/500/Put_GlueVaR_IS_500_sim_saved\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#Market and option parameters as in section 4.1 of 'Assessing Asset-Liability Risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "s_0 = 100\r\n",
    "r = 0.01\r\n",
    "mu = 0.05\r\n",
    "sigma = 0.2\r\n",
    "tau = 1/52\r\n",
    "T = 1/3\r\n",
    "K = 100\r\n",
    "\r\n",
    "#Risk measure parameteres\r\n",
    "alpha_Glue = 0.95\r\n",
    "beta_Glue = 0.995\r\n",
    "omega_Glue = np.array([1/3,1/3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Sizes for training set, validation set, test set, and set size for Monte Carlo estimation of the risk measures\r\n",
    "M_1 = 1500000\r\n",
    "M_2 = 500000\r\n",
    "M_3 = 500000\r\n",
    "M_MC = 500000\r\n",
    "#size of the set of data points used to calculate an IS density\r\n",
    "M_IS = 500000\r\n",
    "#quantile for which the IS density will be computed\r\n",
    "alpha_IS = 0.975"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Function for calculating simulated values of S_tau and simulated payoffs P_T from simulations of standard normal random variables\r\n",
    "def data_gen(Z,V):\r\n",
    "    #simulate S_tau under P\r\n",
    "    S_tau = s_0 * np.exp( (mu-0.5*sigma**2)*tau + sigma*np.sqrt(tau)*Z)\r\n",
    "    #simulate S_T given S_tau under Q\r\n",
    "    S_T = S_tau * np.exp( (r-0.5*sigma**2)*(T-tau) + sigma*np.sqrt(T-tau)*V)\r\n",
    "    #calculate corresponding discounted payoffs\r\n",
    "    P_T = np.exp(-r*(T-tau)) * np.maximum(K-S_T,0)\r\n",
    "    return S_tau, P_T\r\n",
    "\r\n",
    "#The density function of Z\r\n",
    "def f(y):\r\n",
    "    return stats.norm.pdf(y, loc=0, scale=1)\r\n",
    "\r\n",
    "#The density function of Z_\\theta (note that \\theta is replaced by x; this is needed for the least-squares solver to work)\r\n",
    "def f_theta(x, y):\r\n",
    "    return stats.norm.pdf(y, loc=x[0], scale=x[1])\r\n",
    "\r\n",
    "#This function describes the approximation of the expression inside the sum of m_2(theta)\r\n",
    "def g_q_alpha_hat_reweighted(x,L,q_alpha_hat):\r\n",
    "    return np.sqrt(f(y=L[:,0])/f_theta(y=L[:,0],x=x))*(L[:,-1]>q_alpha_hat)\r\n",
    "\r\n",
    "#bounds for the IS density parameters (for the mean parameter no bound is necessary, the standard deviation however needs to be non-negative)\r\n",
    "bnds_lower = np.array([-np.inf,0])\r\n",
    "bnds_upper = np.array([np.inf,np.inf])\r\n",
    "\r\n",
    "#function that calculates GlueVaR\r\n",
    "def GlueVaR_IS(omega, L, alpha, beta, w):\r\n",
    "    j_beta = 0\r\n",
    "    w_sum_tmp = 0\r\n",
    "    while(w_sum_tmp <= (1-beta)):\r\n",
    "        w_sum_tmp += w[j_beta]\r\n",
    "        j_beta += 1\r\n",
    "        \r\n",
    "    j_alpha = j_beta\r\n",
    "    while(w_sum_tmp <= (1-alpha)):\r\n",
    "        w_sum_tmp += w[j_alpha]\r\n",
    "        j_alpha += 1\r\n",
    "        \r\n",
    "    ES_beta = 1/(1-beta) * np.sum(w[0:j_beta-1]*L[0:j_beta-1]) + ( 1 - (1 / (1-beta)) * np.sum(w[0:j_beta-1]) )*L[j_beta]\r\n",
    "    ES_alpha = 1/(1-alpha) * np.sum(w[0:j_alpha-1]*L[0:j_alpha-1]) + ( 1 - (1 / (1-alpha)) * np.sum(w[0:j_alpha-1]) )*L[j_alpha]\r\n",
    "    VaR_alpha = L[j_alpha]\r\n",
    "\r\n",
    "    return omega[0]*ES_beta + omega[1]*ES_alpha + (1-omega[0]-omega[1])*VaR_alpha"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for j in range(100):\r\n",
    "    #Generating realisations of standard normal random variables\r\n",
    "    Z_train = np.random.normal(loc=0, scale=1, size=M_1)\r\n",
    "    V_train = np.random.normal(loc=0, scale=1, size=M_1)\r\n",
    "    Z_val = np.random.normal(loc=0, scale=1, size=M_2)\r\n",
    "    V_val = np.random.normal(loc=0, scale=1, size=M_2)\r\n",
    "    Z_test = np.random.normal(loc=0, scale=1, size=M_3)\r\n",
    "    V_test = np.random.normal(loc=0, scale=1, size=M_3)\r\n",
    "    Z_MC = np.random.normal(loc=0, scale=1, size=M_MC)\r\n",
    "    V_MC = np.random.normal(loc=0, scale=1, size=M_MC)\r\n",
    "\r\n",
    "    Z_IS = np.random.normal(loc=0, scale=1, size=M_IS)\r\n",
    "    V_IS = np.random.normal(loc=0, scale=1, size=M_IS)\r\n",
    "    S_tau_IS, P_T_IS = data_gen(Z=Z_IS, V=V_IS)\r\n",
    "\r\n",
    "\r\n",
    "    #define and compile neural network model, setup as in section 4.1 of 'Assessing Asset-Liability Risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "    bi_IS = np.log( np.sum(P_T_IS)/len(P_T_IS))\r\n",
    "    model_IS = tf.keras.models.Sequential([\r\n",
    "        tf.keras.layers.BatchNormalization(input_shape=(1,)),\r\n",
    "        tf.keras.layers.Dense(5, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(1, activation='exponential', bias_initializer=tf.keras.initializers.Constant(value=bi_IS))])\r\n",
    "    model_IS.compile(loss='mse', optimizer='adam', metrics=['mse'])\r\n",
    "    #train the neural network\r\n",
    "    b=model_IS.fit(x=S_tau_IS, y=P_T_IS, epochs=40, batch_size=10000, verbose=0)\r\n",
    "\r\n",
    "    #computation of \\theta^*_{NN}\r\n",
    "    L_IS = np.column_stack((Z_IS, model_IS.predict(S_tau_IS)[:,0]))\r\n",
    "    L_IS = L_IS[L_IS[:,-1].argsort()[::-1]]\r\n",
    "    q_alpha_IS_hat = L_IS[int(M_IS*(1-alpha_IS)-1), -1]\r\n",
    "    print('q_alpha_IS_hat_NN:', q_alpha_IS_hat)\r\n",
    "    \r\n",
    "    IS_NN = optimize.least_squares(g_q_alpha_hat_reweighted, x0=np.array([0,1]), bounds=(bnds_lower,bnds_upper), args=(L_IS, q_alpha_IS_hat)).x\r\n",
    "    print('IS_NN:', IS_NN)\r\n",
    "    #a runtime warning may occur here but in all trial runs this was not of concern as the optimization algorithm still executed successfully and the result was meaningful\r\n",
    "\r\n",
    "    #calculating DT(Z,\\theta^*_{NN})\r\n",
    "    Z_train_NN = Z_train*IS_NN[1] + IS_NN[0]\r\n",
    "    Z_val_NN = Z_val*IS_NN[1] + IS_NN[0]\r\n",
    "    Z_test_NN = Z_test*IS_NN[1] + IS_NN[0]\r\n",
    "    Z_MC_NN = Z_MC*IS_NN[1] + IS_NN[0]\r\n",
    "    #calculating the risk factors under the IS distribution and corresponding option prices\r\n",
    "    S_tau_train_NN, P_T_train_NN = data_gen(Z=Z_train_NN, V=V_train)\r\n",
    "    S_tau_val_NN, P_T_val_NN = data_gen(Z=Z_val_NN, V=V_val)\r\n",
    "    S_tau_test_NN, P_T_test_NN = data_gen(Z=Z_test_NN, V=V_test)\r\n",
    "    S_tau_MC_NN, P_T_MC_NN = data_gen(Z=Z_MC_NN, V=V_MC)\r\n",
    "\r\n",
    "    #define and compile neural network model, setup as in section 4.1 of 'Assessing Asset-Liability Risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "    bi = np.log( np.sum(P_T_train_NN)/len(P_T_train_NN))\r\n",
    "    model = tf.keras.models.Sequential([\r\n",
    "        tf.keras.layers.BatchNormalization(input_shape=(1,)),\r\n",
    "        tf.keras.layers.Dense(5, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(1, activation='exponential', bias_initializer=tf.keras.initializers.Constant(value=bi))])\r\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\r\n",
    "    #train the model\r\n",
    "    hist = model.fit(x=S_tau_train_NN, y=P_T_train_NN, epochs=40, batch_size=10000, validation_data=(S_tau_val_NN,P_T_val_NN), verbose=0)\r\n",
    "\r\n",
    "    #Calculating the parts of the test set that fall into B_1 and B_2\r\n",
    "    s_40 = s_0 * np.exp( (mu-0.5*sigma**2)*tau + sigma*np.sqrt(tau)*stats.norm.ppf(0.4, loc=0, scale=1))\r\n",
    "    s_70 = s_0 * np.exp( (mu-0.5*sigma**2)*tau + sigma*np.sqrt(tau)*stats.norm.ppf(0.7, loc=0, scale=1))\r\n",
    "    B_1_NN = S_tau_test_NN < s_40\r\n",
    "    B_2_NN = S_tau_test_NN > s_70\r\n",
    "\r\n",
    "    #computation of the metrics (a), (b), (c) with B_1 and (c) with B_2 for the neural network\r\n",
    "    P_T_pred_NN = model.predict(S_tau_test_NN)[:,0]\r\n",
    "    mse_train_NN = hist.history['mse'][-1]\r\n",
    "    mse_val_NN = hist.history['val_mse'][-1]\r\n",
    "    mc_tmp = P_T_pred_NN - P_T_test_NN\r\n",
    "    metric_a_NN = np.sum(mc_tmp)/len(P_T_test_NN)\r\n",
    "    metric_b_NN = np.sum((mc_tmp)*P_T_pred_NN)/len(P_T_test_NN)\r\n",
    "    metric_c_B_1_NN = np.sum(mc_tmp[B_1_NN])/len(P_T_test_NN)\r\n",
    "    metric_c_B_2_NN = np.sum(mc_tmp[B_2_NN])/len(P_T_test_NN)\r\n",
    "\r\n",
    "    #computation of GlueVaR using the neural network\r\n",
    "    P_T_pred_MC_NN = model.predict(S_tau_MC_NN)[:,0]\r\n",
    "    MC = np.column_stack((Z_MC_NN, P_T_pred_MC_NN))\r\n",
    "    MC_sort = MC[MC[:,-1].argsort()[::-1]]\r\n",
    "    w = f(MC_sort[:,0])/(M_MC*f_theta(y=MC_sort[:,0], x=IS_NN))\r\n",
    "\r\n",
    "    GlueVaR_hat_NN = GlueVaR_IS(omega=omega_Glue, L=MC_sort[:,-1], alpha=alpha_Glue, beta=beta_Glue, w=w)\r\n",
    "    print('GlueVaR_hat_NN:',GlueVaR_hat_NN)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    #define and train random forest with the same hyperparameter as in Put_GlueVaR_IS_500.ipynb\r\n",
    "    rfr_IS = RandomForestRegressor(n_estimators=160, criterion='squared_error', min_samples_leaf=250, bootstrap=True, verbose=0, n_jobs=-1)\r\n",
    "    rfr_IS.fit(X=S_tau_IS.reshape(-1,1), y=P_T_IS)\r\n",
    "\r\n",
    "    #computation of \\theta^*_{RF}\r\n",
    "    L_IS = np.column_stack((Z_IS, rfr_IS.predict(S_tau_IS.reshape(-1,1))))\r\n",
    "    L_IS = L_IS[L_IS[:,-1].argsort()[::-1]]\r\n",
    "    q_alpha_IS_hat = L_IS[int(M_IS*(1-alpha_IS)-1), -1]\r\n",
    "    print('q_alpha_IS_hat_RF:', q_alpha_IS_hat)\r\n",
    "    \r\n",
    "    IS_RF = optimize.least_squares(g_q_alpha_hat_reweighted, x0=np.array([0,1]), bounds=(bnds_lower,bnds_upper), args=(L_IS, q_alpha_IS_hat)).x\r\n",
    "    print('IS_RF:', IS_RF)\r\n",
    "    #a runtime warning may occur here but in all trial runs this was not of concern as the optimization algorithm still executed successfully and the result was meaningful\r\n",
    "\r\n",
    "    #calculating DT(Z,\\theta^*_{RF})\r\n",
    "    Z_train_RF = Z_train*IS_RF[1] + IS_RF[0]\r\n",
    "    Z_val_RF = Z_val*IS_RF[1] + IS_RF[0]\r\n",
    "    Z_test_RF = Z_test*IS_RF[1] + IS_RF[0]\r\n",
    "    Z_MC_RF = Z_MC*IS_RF[1] + IS_RF[0]\r\n",
    "    #calculating the risk factors under the IS distribution and corresponding option prices\r\n",
    "    S_tau_train_RF, P_T_train_RF = data_gen(Z=Z_train_RF, V=V_train)\r\n",
    "    S_tau_val_RF, P_T_val_RF = data_gen(Z=Z_val_RF, V=V_val)\r\n",
    "    S_tau_test_RF, P_T_test_RF = data_gen(Z=Z_test_RF, V=V_test)\r\n",
    "    S_tau_MC_RF, P_T_MC_RF = data_gen(Z=Z_MC_RF, V=V_MC)\r\n",
    "    \r\n",
    "    #hypyerparameter tuning with orientation around the value found in Put_GlueVaR_IS_500.ipynb\r\n",
    "    min_samples_leaf_list = [5400,5600,5800]\r\n",
    "    opt_param = 0\r\n",
    "    opt_score = np.inf\r\n",
    "\r\n",
    "    for min_samples_leaf in min_samples_leaf_list:\r\n",
    "        rfr_tuning = RandomForestRegressor(n_estimators=160, min_samples_leaf=min_samples_leaf, bootstrap=True, criterion='squared_error', verbose=0, n_jobs=-1)\r\n",
    "        rfr_tuning.fit(X=S_tau_train_RF.reshape(-1,1), y=P_T_train_RF)\r\n",
    "        score = mean_squared_error(y_true=P_T_val_RF, y_pred=rfr_tuning.predict(S_tau_val_RF.reshape(-1,1)))\r\n",
    "        if score < opt_score:\r\n",
    "            opt_param = min_samples_leaf\r\n",
    "            opt_score = score\r\n",
    "\r\n",
    "    #definition and training of a random forest with hyperparameters found through grid search\r\n",
    "    rfr = RandomForestRegressor(n_estimators=400, criterion='squared_error', min_samples_leaf=int(opt_param), bootstrap=True, verbose=0, warm_start=True, n_jobs=-1)\r\n",
    "    rfr.fit(X=S_tau_train_RF.reshape(-1,1), y=P_T_train_RF)\r\n",
    "\r\n",
    "    #Calculating the parts of the test set that fall into B_1 and B_2\r\n",
    "    s_40 = s_0 * np.exp( (mu-0.5*sigma**2)*tau + sigma*np.sqrt(tau)*stats.norm.ppf(0.4, loc=0, scale=1))\r\n",
    "    s_70 = s_0 * np.exp( (mu-0.5*sigma**2)*tau + sigma*np.sqrt(tau)*stats.norm.ppf(0.7, loc=0, scale=1))\r\n",
    "    B_1_RF = S_tau_test_RF < s_40\r\n",
    "    B_2_RF = S_tau_test_RF > s_70\r\n",
    "\r\n",
    "    #computation of the metrics (a), (b), (c) with B_1 and (c) with B_2 and training/validation MSE for the random forest\r\n",
    "    mse_train_RF = mean_squared_error(y_pred=rfr.predict(S_tau_train_RF.reshape(-1,1)),y_true=P_T_train_RF)\r\n",
    "    P_T_pred_RF = rfr.predict(S_tau_test_RF.reshape(-1,1))\r\n",
    "    mse_val_RF = mean_squared_error(y_pred=P_T_pred_RF,y_true=P_T_test_RF)\r\n",
    "    mc_tmp = P_T_pred_RF - P_T_test_RF\r\n",
    "    metric_a_RF = np.sum(mc_tmp)/len(P_T_test_RF)\r\n",
    "    metric_b_RF = np.sum((mc_tmp)*P_T_pred_RF)/len(P_T_test_RF)\r\n",
    "    metric_c_B_1_RF = np.sum(mc_tmp[B_1_RF])/len(P_T_test_RF)\r\n",
    "    metric_c_B_2_RF = np.sum(mc_tmp[B_2_RF])/len(P_T_test_RF)\r\n",
    "\r\n",
    "    #computation of GlueVaR using the random forest\r\n",
    "    P_T_pred_MC_RF = rfr.predict(S_tau_MC_RF.reshape(-1,1))\r\n",
    "    MC = np.column_stack((Z_MC_RF, P_T_pred_MC_RF))\r\n",
    "    MC_sort = MC[MC[:,-1].argsort()[::-1]]\r\n",
    "    w = f(MC_sort[:,0])/(M_MC*f_theta(y=MC_sort[:,0], x=IS_RF))\r\n",
    "\r\n",
    "    GlueVaR_hat_RF = GlueVaR_IS(omega=omega_Glue, L=MC_sort[:,-1], alpha=alpha_Glue, beta=beta_Glue, w=w)\r\n",
    "    print('GlueVaR_hat_RF:',GlueVaR_hat_RF)\r\n",
    "\r\n",
    "\r\n",
    "    #save results for further evaluation\r\n",
    "    output = np.array([[mse_train_NN,mse_val_NN,metric_a_NN,metric_b_NN,metric_c_B_1_NN,metric_c_B_2_NN,IS_NN[0],IS_NN[1],GlueVaR_hat_NN],\r\n",
    "                      [mse_train_RF,mse_val_RF,metric_a_RF,metric_b_RF,metric_c_B_1_RF,metric_c_B_2_RF,IS_RF[0],IS_RF[1],GlueVaR_hat_RF]])\r\n",
    "\r\n",
    "    joblib.dump(output,filepath+'output_'+str(run)+'_'+str(j)+'.joblib')\r\n",
    "    #prints just for checking while the notebook is running\r\n",
    "    print(j)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-11-08 16:48:25.164929: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bwhpc/common/devel/cuda/11.4/lib64:/opt/bwhpc/common/compiler/gnu/10.2.0/lib:/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-10.2/lib:/opt/bwhpc/common/mpi/openmpi/4.1.1-gnu-10.2/lib64\n",
      "2021-11-08 16:48:25.171525: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-08 16:48:25.171545: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (uc2n526.localdomain): /proc/driver/nvidia/version does not exist\n",
      "2021-11-08 16:48:25.191555: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-08 16:48:27.047151: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "q_alpha_IS_hat_NN: 7.2896552085876465\n",
      "IS_NN: [-2.27088594  0.45638948]\n",
      "GlueVaR_hat_NN: 7.704406129246371\n",
      "q_alpha_IS_hat_RF: 7.299054183935941\n",
      "IS_RF: [-2.06477334  0.50325792]\n",
      "GlueVaR_hat_RF: 7.687677847336836\n",
      "0\n",
      "q_alpha_IS_hat_NN: 6.953082084655762\n",
      "IS_NN: [-2.27680407  0.46994385]\n",
      "GlueVaR_hat_NN: 7.666063965543023\n",
      "q_alpha_IS_hat_RF: 7.299486060808688\n",
      "IS_RF: [-2.06846565  0.51777235]\n",
      "GlueVaR_hat_RF: 7.694167619561977\n",
      "1\n",
      "q_alpha_IS_hat_NN: 7.202442169189453\n",
      "IS_NN: [-2.27640136  0.43853723]\n",
      "GlueVaR_hat_NN: 7.685252685844346\n",
      "q_alpha_IS_hat_RF: 7.337466837877493\n",
      "IS_RF: [-2.07049731  0.49488105]\n",
      "GlueVaR_hat_RF: 7.668184220194778\n",
      "2\n",
      "q_alpha_IS_hat_NN: 7.201234340667725\n",
      "IS_NN: [-2.26978622  0.482779  ]\n",
      "GlueVaR_hat_NN: 7.7428325499889885\n",
      "q_alpha_IS_hat_RF: 7.41142766259769\n",
      "IS_RF: [-2.03324199  0.53117133]\n",
      "GlueVaR_hat_RF: 7.694088753586384\n",
      "3\n",
      "q_alpha_IS_hat_NN: 7.271740436553955\n",
      "IS_NN: [-2.27643407  0.48444054]\n",
      "GlueVaR_hat_NN: 7.693428091724398\n",
      "q_alpha_IS_hat_RF: 7.3890024691091565\n",
      "IS_RF: [-2.12265387  0.51566389]\n",
      "GlueVaR_hat_RF: 7.715181994345885\n",
      "4\n",
      "q_alpha_IS_hat_NN: 7.277640342712402\n",
      "IS_NN: [-2.27100014  0.47449825]\n",
      "GlueVaR_hat_NN: 7.707278430792442\n",
      "q_alpha_IS_hat_RF: 7.3638331029228805\n",
      "IS_RF: [-2.15061457  0.49934946]\n",
      "GlueVaR_hat_RF: 7.699935064369814\n",
      "5\n",
      "q_alpha_IS_hat_NN: 7.120316028594971\n",
      "IS_NN: [-2.27221943  0.49667805]\n",
      "GlueVaR_hat_NN: 7.698169382876646\n",
      "q_alpha_IS_hat_RF: 7.471150986239914\n",
      "IS_RF: [-2.09610724  0.52950676]\n",
      "GlueVaR_hat_RF: 7.689478132040986\n",
      "6\n",
      "q_alpha_IS_hat_NN: 7.203389644622803\n",
      "IS_NN: [-2.27390089  0.45448194]\n",
      "GlueVaR_hat_NN: 7.653587258683427\n",
      "q_alpha_IS_hat_RF: 7.416683718343094\n",
      "IS_RF: [-2.04315797  0.50928598]\n",
      "GlueVaR_hat_RF: 7.6903424694909805\n",
      "7\n",
      "q_alpha_IS_hat_NN: 7.229701042175293\n",
      "IS_NN: [-2.26502621  0.5064293 ]\n",
      "GlueVaR_hat_NN: 7.672944114263338\n",
      "q_alpha_IS_hat_RF: 7.4454358680967685\n",
      "IS_RF: [-2.07210792  0.54540062]\n",
      "GlueVaR_hat_RF: 7.688728171214811\n",
      "8\n",
      "q_alpha_IS_hat_NN: 6.812199115753174\n",
      "IS_NN: [-2.28166895  0.44314564]\n",
      "GlueVaR_hat_NN: 7.6929326738082295\n",
      "q_alpha_IS_hat_RF: 7.400752512640078\n",
      "IS_RF: [-2.01521765  0.51892189]\n",
      "GlueVaR_hat_RF: 7.6993087960850275\n",
      "9\n",
      "q_alpha_IS_hat_NN: 7.550334930419922\n",
      "IS_NN: [-2.27095792  0.46582547]\n",
      "GlueVaR_hat_NN: 7.674654171725589\n",
      "q_alpha_IS_hat_RF: 7.347312312627475\n",
      "IS_RF: [-2.04073349  0.51992903]\n",
      "GlueVaR_hat_RF: 7.678572543351614\n",
      "10\n",
      "q_alpha_IS_hat_NN: 7.307718753814697\n",
      "IS_NN: [-2.26686648  0.46407254]\n",
      "GlueVaR_hat_NN: 7.71754938073591\n",
      "q_alpha_IS_hat_RF: 7.394737380958022\n",
      "IS_RF: [-2.08468945  0.5068926 ]\n",
      "GlueVaR_hat_RF: 7.701791833459186\n",
      "11\n",
      "q_alpha_IS_hat_NN: 7.3759284019470215\n",
      "IS_NN: [-2.28101055  0.45980394]\n",
      "GlueVaR_hat_NN: 7.668867298492417\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/slurm_tmpdir/job_20164529/ipykernel_741285/2651631504.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mrfr_IS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mrfr_IS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mS_tau_IS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mP_T_IS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mL_IS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_IS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrfr_IS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_tau_IS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    442\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b88b057cf34efecd776605b6493619ffcb4d411e0887d16b1b551b502979a3a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}