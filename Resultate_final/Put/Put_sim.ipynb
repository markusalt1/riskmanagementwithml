{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Results were produced in stints. This is the number of the last stint.\r\n",
    "run = 3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from scipy import stats\r\n",
    "import joblib\r\n",
    "\r\n",
    "#folder for saving results\r\n",
    "filepath = \".../Resultate_final/Put/saved_sim/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#Market and option parameters as in section 4.1 of 'Assessing Asset-Liability Risk with Neural Networks' (Cheridito, Ery, WÃ¼thrich 2020)\r\n",
    "s_0 = 100\r\n",
    "r = 0.01\r\n",
    "mu = 0.05\r\n",
    "sigma = 0.2\r\n",
    "tau = 1/52\r\n",
    "T = 1/3\r\n",
    "K = 100\r\n",
    "\r\n",
    "#Confidence levels and parameters for Value-at-Risk, Expected Shortfall and GlueVaR\r\n",
    "alpha_VaR = 0.995\r\n",
    "alpha_ES = 0.99\r\n",
    "alpha_Glue = 0.95\r\n",
    "beta_Glue = 0.995\r\n",
    "omega_Glue = np.array([1/3,1/3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Sizes for training set, validation set, test set, and set size for Monte Carlo estimation of the risk measures\r\n",
    "M_1 = 1500000\r\n",
    "M_2 = 500000\r\n",
    "M_3 = 500000\r\n",
    "M_MC = 500000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Function for calculating simulated values of S_tau and simulated payoffs P_T from simulations of standard normal random variables\r\n",
    "def data_gen(Z,V):\r\n",
    "    #simulate S_tau under P\r\n",
    "    S_tau = s_0 * np.exp( (mu-0.5*sigma**2)*tau + sigma*np.sqrt(tau)*Z)\r\n",
    "    #Simulate S_T given S_tau under Q\r\n",
    "    S_T = S_tau * np.exp( (r-0.5*sigma**2)*(T-tau) + sigma*np.sqrt(T-tau)*V)\r\n",
    "    #Calculate corresponding simulated discounted payoffs\r\n",
    "    P_T = np.exp(-r*(T-tau)) * np.maximum(K-S_T,0)\r\n",
    "    return S_tau, P_T\r\n",
    "\r\n",
    "#Function for the computation of GlueVaR\r\n",
    "def GlueVaR(omega, L, alpha, beta):\r\n",
    "    j_beta = int(len(L)*(1-beta))-1\r\n",
    "    j_alpha = int(len(L)*(1-alpha))-1\r\n",
    "\r\n",
    "    ES_beta = 1/(1-beta) * np.sum(L[0:j_beta-1])/len(L) + ( 1 - (j_beta-1)/((1-beta)*len(L)) )*L[j_beta]\r\n",
    "    ES_alpha = 1/(1-alpha) * np.sum(L[0:j_alpha-1])/len(L) + ( 1 - (j_alpha-1)/((1-alpha)*len(L)) )*L[j_alpha]\r\n",
    "    VaR_alpha = L[j_alpha]\r\n",
    "\r\n",
    "    return omega[0]*ES_beta + omega[1]*ES_alpha + (1-omega[0]-omega[1])*VaR_alpha"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for j in range(17):\r\n",
    "    #Generating simulations of standard normal random variables for training set, validation set, test set, set for Monte Carlo estimation of risk measures\r\n",
    "    Z_train = np.random.normal(loc=0, scale=1, size=M_1)\r\n",
    "    V_train = np.random.normal(loc=0, scale=1, size=M_1)\r\n",
    "    Z_val = np.random.normal(loc=0, scale=1, size=M_2)\r\n",
    "    V_val = np.random.normal(loc=0, scale=1, size=M_2)\r\n",
    "    Z_test = np.random.normal(loc=0, scale=1, size=M_3)\r\n",
    "    V_test = np.random.normal(loc=0, scale=1, size=M_3)\r\n",
    "    Z_MC = np.random.normal(loc=0, scale=1, size=M_MC)\r\n",
    "    V_MC = np.random.normal(loc=0, scale=1, size=M_MC)\r\n",
    "\r\n",
    "    #Calculate the risk factor S_tau and the corresponding simulated payoffs P_T\r\n",
    "    S_tau_train, P_T_train = data_gen(Z=Z_train, V=V_train)\r\n",
    "    S_tau_val, P_T_val = data_gen(Z=Z_val, V=V_val)\r\n",
    "    S_tau_test, P_T_test = data_gen(Z=Z_test, V=V_test)\r\n",
    "    S_tau_MC, P_T_MC = data_gen(Z=Z_MC, V=V_MC)\r\n",
    "\r\n",
    "    #define and compile neural network model, setup as in paper\r\n",
    "    bi = np.log( np.sum(P_T_train)/len(P_T_train))\r\n",
    "    model = tf.keras.models.Sequential([\r\n",
    "        tf.keras.layers.BatchNormalization(input_shape=(1,)),\r\n",
    "        tf.keras.layers.Dense(5, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(1, activation='exponential', bias_initializer=tf.keras.initializers.Constant(value=bi))])\r\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\r\n",
    "    #training of the neural network\r\n",
    "    hist = model.fit(x=S_tau_train, y=P_T_train, epochs=40, batch_size=10000, validation_data=(S_tau_val,P_T_val), verbose=0)\r\n",
    "\r\n",
    "    #training of the random forest, hyperparameters taken from Put.ipynb\r\n",
    "    rfr = RandomForestRegressor(n_estimators=400, criterion='squared_error', min_samples_leaf=2000, bootstrap=True, verbose=0, warm_start=True, n_jobs=-1)\r\n",
    "    rfr.fit(X=S_tau_train.reshape(-1,1), y=P_T_train)\r\n",
    "\r\n",
    "\r\n",
    "    #Calculating the parts of the test set that fall into B_1 and B_2\r\n",
    "    s_40 = s_0 * np.exp( (mu-0.5*sigma**2)*tau + sigma*np.sqrt(tau)*stats.norm.ppf(0.4, loc=0, scale=1))\r\n",
    "    s_70 = s_0 * np.exp( (mu-0.5*sigma**2)*tau + sigma*np.sqrt(tau)*stats.norm.ppf(0.7, loc=0, scale=1))\r\n",
    "    B_1 = S_tau_test < s_40\r\n",
    "    B_2 = S_tau_test > s_70\r\n",
    "\r\n",
    "    #computation of the metrics (a), (b), (c) with B_1 and (c) with B_2 for the neural network\r\n",
    "    P_T_pred_NN = model.predict(S_tau_test)[:,0]\r\n",
    "    mse_train_NN = hist.history['mse'][-1]\r\n",
    "    mse_val_NN = hist.history['val_mse'][-1]\r\n",
    "    mc_tmp = P_T_pred_NN - P_T_test\r\n",
    "    metric_a_NN = np.sum(mc_tmp)/len(P_T_test)\r\n",
    "    metric_b_NN = np.sum((mc_tmp)*P_T_pred_NN)/len(P_T_test)\r\n",
    "    metric_c_B_1_NN = np.sum(mc_tmp[B_1])/len(P_T_test)\r\n",
    "    metric_c_B_2_NN = np.sum(mc_tmp[B_2])/len(P_T_test)\r\n",
    "\r\n",
    "    #computation of the metrics (a), (b), (c) with B_1 and (c) with B_2 for the random forest\r\n",
    "    mse_train_RF = mean_squared_error(y_pred=rfr.predict(S_tau_train.reshape(-1,1)),y_true=P_T_train)\r\n",
    "    P_T_pred_RF = rfr.predict(S_tau_test.reshape(-1,1))\r\n",
    "    mse_val_RF = mean_squared_error(y_pred=P_T_pred_RF,y_true=P_T_test)\r\n",
    "    mc_tmp = P_T_pred_RF - P_T_test\r\n",
    "    metric_a_RF = np.sum(mc_tmp)/len(P_T_test)\r\n",
    "    metric_b_RF = np.sum((mc_tmp)*P_T_pred_RF)/len(P_T_test)\r\n",
    "    metric_c_B_1_RF = np.sum(mc_tmp[B_1])/len(P_T_test)\r\n",
    "    metric_c_B_2_RF = np.sum(mc_tmp[B_2])/len(P_T_test)\r\n",
    "\r\n",
    "    #calulation of risk measures\r\n",
    "    #calculating realisations of \\widehat{L}, here denoted by P_T_pred\r\n",
    "    P_T_pred_MC_NN = model.predict(S_tau_MC)[:,0]\r\n",
    "    P_T_pred_MC_RF = rfr.predict(S_tau_MC.reshape(-1,1))\r\n",
    "\r\n",
    "    #sorting observations\r\n",
    "    L_NN = np.sort(P_T_pred_MC_NN)[::-1]\r\n",
    "    L_RF = np.sort(P_T_pred_MC_RF)[::-1]\r\n",
    "\r\n",
    "    #Value-at-Risk\r\n",
    "    j_VaR = int(M_MC*(1-alpha_VaR))-1\r\n",
    "    VaR_hat_NN = L_NN[j_VaR]\r\n",
    "    VaR_hat_RF = L_RF[j_VaR]\r\n",
    "\r\n",
    "    #Expected Shortfall\r\n",
    "    j_ES = int(M_MC*(1-alpha_ES))-1\r\n",
    "    ES_hat_NN = 1/(1-alpha_ES) * np.sum(L_NN[0:j_ES-1])/M_MC + ( 1 - (j_ES-1)/((1-alpha_ES)*M_MC) )*L_NN[j_ES]\r\n",
    "    ES_hat_RF = 1/(1-alpha_ES) * np.sum(L_RF[0:j_ES-1])/M_MC + ( 1 - (j_ES-1)/((1-alpha_ES)*M_MC) )*L_RF[j_ES]\r\n",
    "    \r\n",
    "    #GlueVaR\r\n",
    "    GlueVaR_hat_NN = GlueVaR(omega=omega_Glue, L=L_NN, alpha=alpha_Glue, beta=beta_Glue)\r\n",
    "    GlueVaR_hat_RF = GlueVaR(omega=omega_Glue, L=L_RF, alpha=alpha_Glue, beta=beta_Glue)\r\n",
    "\r\n",
    "    #save results for further evaluation\r\n",
    "    output = np.array([[mse_train_NN,mse_val_NN,metric_a_NN,metric_b_NN,metric_c_B_1_NN,metric_c_B_2_NN,VaR_hat_NN,ES_hat_NN,GlueVaR_hat_NN],\r\n",
    "                      [mse_train_RF,mse_val_RF,metric_a_RF,metric_b_RF,metric_c_B_1_RF,metric_c_B_2_RF,VaR_hat_RF,ES_hat_RF,GlueVaR_hat_RF]])\r\n",
    "\r\n",
    "    joblib.dump(output,filepath+'output_'+str(run)+'_'+str(j)+'.joblib')\r\n",
    "    #prints just for checking while the notebook is running\r\n",
    "    print(j)\r\n",
    "    print(mse_val_NN,mse_val_RF)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-11-09 18:22:38.052858: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-09 18:22:40.177573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30988 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:16:00.0, compute capability: 7.0\n",
      "2021-11-09 18:22:40.179192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30988 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3a:00.0, compute capability: 7.0\n",
      "2021-11-09 18:22:40.180732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30988 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2021-11-09 18:22:40.182224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30988 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n",
      "2021-11-09 18:22:41.095688: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "36.49256896972656 36.36761807049517\n",
      "1\n",
      "36.30271911621094 36.28382218138532\n",
      "2\n",
      "36.44670867919922 36.21964702118515\n",
      "3\n",
      "36.330772399902344 36.38972251224388\n",
      "4\n",
      "36.36061477661133 36.29348043378203\n",
      "5\n",
      "36.4785270690918 36.419567332838916\n",
      "6\n",
      "36.395565032958984 36.47652487810508\n",
      "7\n",
      "36.2723274230957 36.52228017725057\n",
      "8\n",
      "36.422969818115234 36.45520438158813\n",
      "9\n",
      "36.442474365234375 36.50504095241592\n",
      "10\n",
      "36.46348190307617 36.36702382549602\n",
      "11\n",
      "36.43077850341797 36.30834518299279\n",
      "12\n",
      "36.37877655029297 36.385144154775126\n",
      "13\n",
      "36.523704528808594 36.52693616048291\n",
      "14\n",
      "36.29164505004883 36.48368433867632\n",
      "15\n",
      "36.32743453979492 36.47279997999922\n",
      "16\n",
      "36.29365921020508 36.39300426220398\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b88b057cf34efecd776605b6493619ffcb4d411e0887d16b1b551b502979a3a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}