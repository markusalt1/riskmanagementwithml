{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Results were produced in stints. This is the number of the last stint.\r\n",
    "run = 9"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "from sklearn.utils import shuffle\r\n",
    "from scipy import stats\r\n",
    "from scipy import optimize\r\n",
    "import joblib\r\n",
    "\r\n",
    "#folder for saving results\r\n",
    "filepath = \".../Resultate_final/VarAnn/IS/VarAnn_VaR_ES_IS_sim_saved/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#parameters as in section 4.3 of 'Assessing Asset-Liability risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "#and section 5.1 of 'A Least-Squares Monte Carlo Approach to the Estimation of Enterprise Risk' (Ha, Bauer 2019)\r\n",
    "#parameters for q\r\n",
    "q_0 = 4.605\r\n",
    "m = 0.05\r\n",
    "sigma_S = 0.18\r\n",
    "#parameters for r\r\n",
    "r_0 = 0.025\r\n",
    "zeta = 0.25\r\n",
    "gamma = 0.02\r\n",
    "sigma_r = 0.01\r\n",
    "lambd = 0.02\r\n",
    "gamma_bar = gamma - (lambd*sigma_r)/zeta\r\n",
    "#parameters for mu_(55+t)\r\n",
    "mu_55 = 0.01\r\n",
    "kappa = 0.07\r\n",
    "sigma_mu = 0.0012\r\n",
    "#parameters for brownian motion\r\n",
    "rho_12 = -0.3\r\n",
    "rho_13 = 0.06\r\n",
    "rho_23 = -0.04\r\n",
    "cov_mat = np.array([[1,rho_12,rho_13],[rho_12,1,rho_23],[rho_13,rho_23,1]])\r\n",
    "#horizon parameters\r\n",
    "tau = 1\r\n",
    "T = 15\r\n",
    "b = 10.792\r\n",
    "#risk measure parameters for Value-at-Risk and Expected Shortfall\r\n",
    "alpha_VaR = 0.995\r\n",
    "alpha_ES = 0.99"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Sizes for training, validation, test, and set size for Monte Carlo estimation of the risk measures\r\n",
    "M_1 = 1500000\r\n",
    "M_2 = 500000\r\n",
    "M_3 = 500000\r\n",
    "#ignore N or N_2 in the following. Was kept just in case, but not used.\r\n",
    "N_2 = 1\r\n",
    "M_MC = 500000\r\n",
    "#size of the set of data points used to calculate an IS density\r\n",
    "M_IS = 750000\r\n",
    "#quantile for which the IS density will be computed\r\n",
    "alpha_IS = 0.995"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Definition of these functions analogously to the appendix of 'A Least-Squares Monte Carlo Approach to the Estimation of Enterprise Risk' (Ha, Bauer 2019)\r\n",
    "def B_r(t,T):\r\n",
    "    return ((1-np.exp(-zeta*(T-t)))/zeta)\r\n",
    "def B_mu(t,T):\r\n",
    "    return ((np.exp(kappa*(T-t))-1)/kappa)\r\n",
    "def A(t,T):\r\n",
    "    tmp1 = gamma_bar*(B_r(t,T)-(T-t))\r\n",
    "    tmp2 = (sigma_r/zeta)**2 * ((T-t) - 2*B_r(t,T) + (1-np.exp(-2*zeta*(T-t)))/(2*zeta))\r\n",
    "    tmp3 = (sigma_mu/kappa)**2 * ((T-t) - 2*B_mu(t,T) + (np.exp(2*kappa*(T-t))-1)/(2*kappa))\r\n",
    "    tmp4 =  2*rho_23*sigma_r*sigma_mu/(zeta*kappa) * (B_mu(t,T) - (T-t) + B_r(t,T) - (1-np.exp(-(zeta-kappa)*(T-t)))/(zeta-kappa))\r\n",
    "    return np.exp(tmp1+0.5*(tmp2+tmp3+tmp4))\r\n",
    "#Definition of this function analogously to section 4.3 of 'Assessing Asset-Liability risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "def F(t,k,r_t,mu_xt):\r\n",
    "    return (A(t,t+k)*np.exp(-B_r(t,t+k)*r_t - B_mu(t,t+k)*mu_xt))\r\n",
    "\r\n",
    "#parameters of the normal distribution of X_tau according to the appendix of 'A Least-Squares Monte Carlo Approach to the Estimation of Enterprise Risk' (Ha, Bauer 2019)\r\n",
    "mu_q_tau = q_0 + (m-0.5*(sigma_S**2))*tau\r\n",
    "mu_r_tau = r_0*np.exp(-zeta*tau) + gamma*(1-np.exp(-zeta*tau))\r\n",
    "mu_mu_55_tau = mu_55*np.exp(kappa*tau)\r\n",
    "mean_tau = np.array([mu_q_tau, mu_r_tau, mu_mu_55_tau])\r\n",
    "\r\n",
    "cov_q_r_tau = rho_12*sigma_S*sigma_r*B_r(0,tau)\r\n",
    "cov_q_mu_tau = rho_13*sigma_S*sigma_mu*B_mu(0,tau)\r\n",
    "cov_r_mu_tau = rho_23*sigma_r*sigma_mu* ((1-np.exp(-(zeta-kappa)*tau))/(zeta-kappa))\r\n",
    "var_q_tau = (sigma_S**2) * tau\r\n",
    "var_r_tau = (sigma_r**2) * ((1-np.exp(-2*zeta*tau))/(2*zeta))\r\n",
    "var_mu_tau = (sigma_mu**2) * ((np.exp(2*kappa*tau)-1)/(2*kappa))\r\n",
    "cov_mat_tau = np.array([[var_q_tau, cov_q_r_tau, cov_q_mu_tau], [cov_q_r_tau, var_r_tau, cov_r_mu_tau], [cov_q_mu_tau, cov_r_mu_tau, var_mu_tau]])\r\n",
    "C_tau = np.linalg.cholesky(cov_mat_tau)\r\n",
    "\r\n",
    "#variance/covariance parameters of the conditional normal distribution of X_T according to the appendix of 'A Least-Squares Monte Carlo Approach to the Estimation of Enterprise Risk' (Ha, Bauer 2019)\r\n",
    "var_q_T_cond = (sigma_S**2) *(T-tau) + ((sigma_r/zeta)**2) * (T-tau - 2*(1-np.exp(-zeta*(T-tau)))/zeta + (1-np.exp(-2*zeta*(T-tau)))/(2*zeta)) + (2*rho_12*sigma_S*sigma_r/zeta) * (T-tau- (1-np.exp(-zeta*(T-tau)))/zeta)\r\n",
    "cov_q_r_T_cond = rho_12*sigma_S*sigma_r*((1-np.exp(-zeta*(T-tau)))/zeta) + ((sigma_r**2)/zeta) * ((1-2*np.exp(-zeta*(T-tau))+np.exp(-2*zeta*(T-tau)))/(2*zeta))\r\n",
    "cov_q_mu_T_cond = rho_13*sigma_S*sigma_mu *((np.exp(kappa*(T-tau))-1)/kappa) + (rho_23*sigma_r*sigma_mu/zeta) * ((np.exp(kappa*(T-tau))-1)/kappa - (1-np.exp(-(zeta-kappa)*(T-tau)))/(zeta-kappa))\r\n",
    "var_r_T_cond = (sigma_r**2) * ((1-np.exp(-2*zeta*(T-tau)))/(2*zeta))\r\n",
    "cov_r_mu_T_cond = rho_23*sigma_r*sigma_mu*((1-np.exp(-(zeta-kappa)*(T-tau)))/(zeta-kappa))\r\n",
    "var_mu_T_cond = (sigma_mu**2) * ((np.exp(2*kappa*(T-tau))-1)/(2*kappa))\r\n",
    "cov_mat_T_cond = np.array([[var_q_T_cond, cov_q_r_T_cond, cov_q_mu_T_cond], [cov_q_r_T_cond, var_r_T_cond, cov_r_mu_T_cond], [cov_q_mu_T_cond, cov_r_mu_T_cond, var_mu_T_cond]])\r\n",
    "\r\n",
    "#function for generating simulated risk factors X_tau and corresponding payments Y from multivariate standard normal random variables\r\n",
    "def data_gen(Z,V):\r\n",
    "    #simulation of X_tau\r\n",
    "    X_tau = np.transpose(np.matmul(C_tau,np.transpose(Z))) + np.tile(mean_tau, (len(Z),1))\r\n",
    "\r\n",
    "    #simulation of X_T given X_tau\r\n",
    "    mu_q_T_cond = X_tau[:,0] + B_r(tau,T)*X_tau[:,1] + (gamma_bar - (sigma_r/zeta)**2)*(T-tau - (1-np.exp(-zeta*(T-tau)))/zeta) + 0.5*((sigma_r/zeta)**2) * ((1-np.exp(-zeta*(T-tau)))/zeta - (np.exp(-zeta*(T-tau))-np.exp(-2*zeta*(T-tau)))/zeta) - ((rho_23*sigma_r*sigma_mu)/kappa) * ( (np.exp(kappa*(T-tau))-1)/(kappa*(zeta-kappa)) - (np.exp(kappa*(T-tau))-np.exp(-(zeta-kappa)*(T-tau)))/(zeta*(zeta-kappa)) - (1/zeta) * (T-tau - (1-np.exp(-zeta*(T-tau)))/zeta)) -0.5*(sigma_S**2) * (T-tau) - (rho_12*sigma_S*sigma_r/zeta) * (T-tau - (1-np.exp(-zeta*(T-tau)))/zeta) - (rho_13*sigma_S*sigma_mu/kappa) * ((np.exp(kappa*(T-tau))-1)/kappa -T+tau)\r\n",
    "    mu_r_T_cond = np.exp(-zeta*(T-tau))*X_tau[:,1] + (gamma_bar-(sigma_r/zeta)**2)*(1-np.exp(-zeta*(T-tau))) + 0.5*((sigma_r/zeta)**2) *(1-np.exp(-2*zeta*(T-tau))) - (rho_23*sigma_r*sigma_mu/kappa) * ((1-np.exp(-(zeta-kappa)*(T-tau)))/(zeta-kappa) - (1-np.exp(-zeta*(T-tau)))/zeta)\r\n",
    "    mu_mu_T_cond = np.exp(kappa*(T-tau))*X_tau[:,2] - (rho_23*sigma_r*sigma_mu/zeta) * ((np.exp(kappa*(T-tau))-1)/kappa - (1-np.exp(-(zeta-kappa)*(T-tau)))/(zeta-kappa)) - ((sigma_mu**2)/kappa) * ((np.exp(2*kappa*(T-tau))-1)/(2*kappa) - (np.exp(kappa*(T-tau))-1)/kappa)\r\n",
    "    mean_T_cond = np.array([mu_q_T_cond, mu_r_T_cond, mu_mu_T_cond])\r\n",
    "\r\n",
    "    X_T = V + np.transpose(mean_T_cond)\r\n",
    "    \r\n",
    "    #calculation of Y from X_T and X_tau\r\n",
    "    Y = F(t=tau, k=T-tau, r_t=X_tau[:,1], mu_xt=X_tau[:,2]) * np.maximum(np.exp(X_T[:,0]), b*np.sum([F(t=T, k=i, r_t=X_T[:,1], mu_xt=X_T[:,2]) for i in range(1,51)], axis=0))\r\n",
    "    return X_tau, Y\r\n",
    "\r\n",
    "#the function DT(Z,\\theta)\r\n",
    "def data_trans_IS(Z,IS):\r\n",
    "    res = np.empty((len(Z),3))\r\n",
    "    for j in range(3):\r\n",
    "        res[:,j] = Z[:,j]*np.sqrt(IS[3+j]) + IS[j]\r\n",
    "    return res\r\n",
    "\r\n",
    "#The density function of Z\r\n",
    "def f(y):\r\n",
    "    return stats.multivariate_normal.pdf(y, mean=np.full(3,0), cov=np.identity(3))\r\n",
    "\r\n",
    "#The density function of Z_\\theta (note that x is interpreted as theta, needed for the least-squares solver to work properly)\r\n",
    "def f_theta(y, x):\r\n",
    "    return stats.multivariate_normal.pdf(y, mean=x[0:3], cov=np.diag(x[3:6]))\r\n",
    "\r\n",
    "#This function describes the approximation of the expression inside the sum of m_2(theta)\r\n",
    "def g_q_alpha_hat_reweighted(x,L,q_alpha_hat):\r\n",
    "    return [(np.sqrt(f(y=L[i,0:3])/f_theta(x=x, y=L[i,0:3])) if L[i,-1]>q_alpha_hat else 0) for i in range(len(L))]\r\n",
    "\r\n",
    "#bounds for the IS density parameters (for the parameters corresponding to the mean no bounds are necessary, the standard deviation parameters however needs to be non-negative)\r\n",
    "bnds_lower = np.array([-np.inf,-np.inf,-np.inf,0,0,0])\r\n",
    "bnds_upper = np.full(6,np.inf)\r\n",
    "bnds = (bnds_lower,bnds_upper)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for j in range(100):\r\n",
    "    #Generating realisations of multivariate standard normal random variables, V correlated\r\n",
    "    Z_IS = np.random.multivariate_normal(mean=np.full(3,0), cov=np.identity(3), size=M_IS)\r\n",
    "    V_IS = np.random.multivariate_normal(mean=np.full(3,0), cov=cov_mat_T_cond, size=M_IS)\r\n",
    "\r\n",
    "    #Calculate the risk factor X_tau and the corresponding simulated payoffs Y\r\n",
    "    X_tau_IS, Y_IS = data_gen(Z=Z_IS, V=V_IS)\r\n",
    "\r\n",
    "    #define and compile neural network model, setup as in section 4.3 of 'Assessing Asset-Liability Risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "    bi_IS = np.log( np.sum(Y_IS)/len(Y_IS))\r\n",
    "    model_IS = tf.keras.models.Sequential([\r\n",
    "        tf.keras.layers.BatchNormalization(input_shape=(3,)),\r\n",
    "        tf.keras.layers.Dense(4, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(4, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(1, activation='exponential', bias_initializer=tf.keras.initializers.Constant(value=bi_IS))])\r\n",
    "    model_IS.compile(loss='mse', optimizer='adam', metrics=['mse'])\r\n",
    "    model_IS.fit(x=X_tau_IS, y=Y_IS, epochs=40, batch_size=10000, verbose=0)\r\n",
    "\r\n",
    "    #Calculate realisations of L_hat from the training data set using the trained neural network\r\n",
    "    L_hat_IS = np.column_stack((Z_IS, model_IS.predict(X_tau_IS)[:,0]))\r\n",
    "    L_hat_IS_sort = L_hat_IS[L_hat_IS[:,-1].argsort()[::-1]]\r\n",
    "\r\n",
    "    #Calculating the corresponding estimator for Value-at-Risk in order to approximate g\r\n",
    "    q_alpha_IS_hat = L_hat_IS_sort[int(M_IS*(1-alpha_IS)-1), -1]\r\n",
    "    print('q_alpha_IS_hat_NN:',q_alpha_IS_hat)\r\n",
    "\r\n",
    "    #Calculating the (hopefully) approximately optimal \\theta^*_{NN} by minimising m_2 using the approximated g\r\n",
    "    IS_NN = optimize.least_squares(g_q_alpha_hat_reweighted, x0=np.concatenate((np.full(3,0),np.full(3,1))), args=(L_hat_IS, q_alpha_IS_hat),bounds=bnds).x\r\n",
    "\r\n",
    "    #define and train a random forest according to the optimal parameters from tuning\r\n",
    "    rfr_IS = RandomForestRegressor(n_estimators=160, criterion='squared_error', max_features=2, min_samples_leaf=1300, bootstrap=True, verbose=0, n_jobs=-1)\r\n",
    "    rfr_IS.fit(X=X_tau_IS, y=Y_IS)\r\n",
    "\r\n",
    "    #Calculate realisations of L_hat from the training data set using the trained random forest\r\n",
    "    L_hat_IS = np.column_stack((Z_IS, rfr_IS.predict(X_tau_IS)))\r\n",
    "    L_hat_IS_sort = L_hat_IS[L_hat_IS[:,-1].argsort()[::-1]]\r\n",
    "\r\n",
    "    #Calculating the corresponding estimator for Value-at-Risk in order to approximate g\r\n",
    "    q_alpha_IS_hat = L_hat_IS_sort[int(M_IS*(1-alpha_IS)-1), -1]\r\n",
    "    print('q_alpha_IS_hat_RF:',q_alpha_IS_hat)\r\n",
    "\r\n",
    "    #Calculating the (hopefully) approximately optimal \\theta^*_{RF} by minimising m_2 using the approximated g\r\n",
    "    IS_RF = optimize.least_squares(g_q_alpha_hat_reweighted, x0=np.concatenate((np.full(3,0),np.full(3,1))), args=(L_hat_IS, q_alpha_IS_hat), bounds=bnds).x\r\n",
    "\r\n",
    "    #print IS density parameters for checking\r\n",
    "    print('IS_NN:',IS_NN)\r\n",
    "    print('IS_RF:',IS_RF)\r\n",
    "    \r\n",
    "    #Generating simulations for multivariate standard normal random variables (uncorrelated for Z and correlated for V) for training set, validation set, test set, set for Monte Carlo estimation of risk measures\r\n",
    "    Z_train = np.random.multivariate_normal(mean=np.full(3,0), cov=np.identity(3), size=M_1)\r\n",
    "    V_train = np.random.multivariate_normal(mean=np.full(3,0), cov=cov_mat_T_cond, size=M_1)\r\n",
    "    Z_val = np.random.multivariate_normal(mean=np.full(3,0), cov=np.identity(3), size=M_2)\r\n",
    "    V_val = np.random.multivariate_normal(mean=np.full(3,0), cov=cov_mat_T_cond, size=M_2)\r\n",
    "    Z_test = np.random.multivariate_normal(mean=np.full(3,0), cov=np.identity(3), size=M_3)\r\n",
    "    V_test = np.random.multivariate_normal(mean=np.full(3,0), cov=cov_mat_T_cond, size=M_3)\r\n",
    "    Z_MC = np.random.multivariate_normal(mean=np.full(3,0), cov=np.identity(3), size=M_MC)\r\n",
    "    V_MC = np.random.multivariate_normal(mean=np.full(3,0), cov=cov_mat_T_cond, size=M_MC)\r\n",
    "\r\n",
    "    #calculate DT(Z,\\theta^*_{NN})\r\n",
    "    Z_train_NN = data_trans_IS(Z_train,IS_NN)\r\n",
    "    Z_val_NN = data_trans_IS(Z_val,IS_NN)\r\n",
    "    Z_test_NN = data_trans_IS(Z_test,IS_NN)\r\n",
    "    Z_MC_NN = data_trans_IS(Z_MC,IS_NN)\r\n",
    "    #calculating the risk factors under the IS distribution and corresponding realised payoffs\r\n",
    "    X_tau_train_NN, Y_train_NN = data_gen(Z=Z_train_NN, V=V_train)\r\n",
    "    X_tau_val_NN, Y_val_NN = data_gen(Z=Z_val_NN, V=V_val)\r\n",
    "    X_tau_test_NN, Y_test_NN = data_gen(Z=Z_test_NN, V=V_test)\r\n",
    "    X_tau_MC_NN, Y_MC_NN = data_gen(Z=Z_MC_NN, V=V_MC)\r\n",
    "\r\n",
    "    #calculate DT(Z,\\theta^*_{RF})\r\n",
    "    Z_train_RF = data_trans_IS(Z_train,IS_RF)\r\n",
    "    Z_val_RF = data_trans_IS(Z_val,IS_RF)\r\n",
    "    Z_test_RF = data_trans_IS(Z_test,IS_RF)\r\n",
    "    Z_MC_RF = data_trans_IS(Z_MC,IS_RF)\r\n",
    "    #calculating the risk factors under the IS distribution and corresponding realised payoffs\r\n",
    "    X_tau_train_RF, Y_train_RF = data_gen(Z=Z_train_RF, V=V_train)\r\n",
    "    X_tau_val_RF, Y_val_RF = data_gen(Z=Z_val_RF, V=V_val)\r\n",
    "    X_tau_test_RF, Y_test_RF = data_gen(Z=Z_test_RF, V=V_test)\r\n",
    "    X_tau_MC_RF, Y_MC_RF = data_gen(Z=Z_MC_RF, V=V_MC)\r\n",
    "    \r\n",
    "    #calculating parameters for the sets B_1 and B_2\r\n",
    "    q_70 = stats.norm.ppf(0.7, loc=mu_q_tau, scale=var_q_tau)\r\n",
    "    q_30 = stats.norm.ppf(0.3, loc=mu_q_tau, scale=var_q_tau)\r\n",
    "    r_70 = stats.norm.ppf(0.7, loc=mu_r_tau, scale=var_r_tau)\r\n",
    "    r_30 = stats.norm.ppf(0.3, loc=mu_r_tau, scale=var_r_tau)\r\n",
    "\r\n",
    "    #calculate the indices of the set B_1 and B_2 for the test set created with the IS density calculated by the neural network\r\n",
    "    B_1_NN = np.apply_along_axis(np.all, axis=1, arr=np.column_stack( (X_tau_test_NN[:,0] > q_70, X_tau_test_NN[:,1] < r_30)) )\r\n",
    "    B_2_NN = np.apply_along_axis(np.all, axis=1, arr=np.column_stack( (X_tau_test_NN[:,0] < q_30, X_tau_test_NN[:,1] > r_70)) )\r\n",
    "\r\n",
    "    #calculate the indices of the set B_1 and B_2 for the test set created with the IS density calculated by the random forest\r\n",
    "    B_1_RF = np.apply_along_axis(np.all, axis=1, arr=np.column_stack( (X_tau_test_RF[:,0] > q_70, X_tau_test_RF[:,1] < r_30)) )\r\n",
    "    B_2_RF = np.apply_along_axis(np.all, axis=1, arr=np.column_stack( (X_tau_test_RF[:,0] < q_30, X_tau_test_RF[:,1] > r_70)) )\r\n",
    "    \r\n",
    "    #define and compile neural network model, setup as in section 4.3 of 'Assessing Asset-Liability Risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "    bi = np.log( np.sum(Y_train_NN)/len(Y_train_NN))\r\n",
    "    model = tf.keras.models.Sequential([\r\n",
    "        tf.keras.layers.BatchNormalization(input_shape=(3,)),\r\n",
    "        tf.keras.layers.Dense(4, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(4, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(1, activation='exponential', bias_initializer=tf.keras.initializers.Constant(value=bi))])\r\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\r\n",
    "    #training the neural network\r\n",
    "    hist = model.fit(x=X_tau_train_NN, y=Y_train_NN, epochs=40, batch_size=10000, validation_data=(X_tau_val_NN,Y_val_NN), verbose=0)\r\n",
    "    \r\n",
    "    #computation of the metrics (a), (b), (c) with B_1 and (c) with B_2 for the neural network\r\n",
    "    Y_pred_NN = model.predict(X_tau_test_NN)[:,0]\r\n",
    "    mse_train_NN = hist.history['mse'][-1]\r\n",
    "    mse_val_NN = hist.history['val_mse'][-1]\r\n",
    "    mc_tmp = Y_pred_NN - Y_test_NN\r\n",
    "    metric_a_NN = np.sum(mc_tmp)/len(Y_test_NN)\r\n",
    "    metric_b_NN = np.sum((mc_tmp)*Y_pred_NN)/len(Y_test_NN)\r\n",
    "    metric_c_B_1_NN = np.sum(mc_tmp[B_1_NN])/len(Y_test_NN)\r\n",
    "    metric_c_B_2_NN = np.sum(mc_tmp[B_2_NN])/len(Y_test_NN)\r\n",
    "\r\n",
    "    #computation of expected payoffs depending on the risk factor X_tau according to the models, i.e. computation of L_hat_i's\r\n",
    "    L_hat_NN = model.predict(X_tau_MC_NN)[:,0]\r\n",
    "    L_hat_c_NN = np.column_stack((Z_MC_NN, L_hat_NN))\r\n",
    "    \r\n",
    "    #calculation of the IS estimator for Value-at-Risk and Expected Shortfall\r\n",
    "    L_hat_c_sort_NN = L_hat_c_NN[L_hat_c_NN[:,-1].argsort()[::-1]]\r\n",
    "    w = f(L_hat_c_sort_NN[:,0:3])/(M_MC*f_theta(x=IS_NN, y=L_hat_c_sort_NN[:,0:3]))\r\n",
    "\r\n",
    "    j_VaR = 0\r\n",
    "    w_sum_tmp = 0\r\n",
    "    while (w_sum_tmp <= (1-alpha_VaR) and j_VaR<M_MC):\r\n",
    "        w_sum_tmp += w[j_VaR]\r\n",
    "        j_VaR += 1\r\n",
    "    VaR_hat_NN = L_hat_c_sort_NN[j_VaR,-1]\r\n",
    "    print('VaR_hat_NN:',VaR_hat_NN)\r\n",
    "    \r\n",
    "    j_ES = 0\r\n",
    "    w_sum_tmp = 0\r\n",
    "    while (w_sum_tmp <= (1-alpha_ES) and j_ES<M_MC):\r\n",
    "        w_sum_tmp += w[j_ES]\r\n",
    "        j_ES += 1\r\n",
    "    ES_hat_NN = (1/(1-alpha_ES)) * np.sum(w[0:j_ES-1]*L_hat_c_sort_NN[0:j_ES-1,-1]) + ( 1 - (1 / (1-alpha_ES)) * np.sum(w[0:j_ES-1]) )*L_hat_c_sort_NN[j_ES,-1]\r\n",
    "    print('ES_hat_NN:',ES_hat_NN)\r\n",
    "    \r\n",
    "    \r\n",
    "    #perform a grid search in order to find the (approximately) best hyperparameter min_samples_leaf\r\n",
    "    #values that will be checked\r\n",
    "    max_features_list = [2]\r\n",
    "    min_samples_leaf_list = [4250,4500,4750]\r\n",
    "    opt_param = np.full(2,0)\r\n",
    "    opt_score = np.inf\r\n",
    "\r\n",
    "    for max_features in max_features_list:\r\n",
    "        for min_samples_leaf in min_samples_leaf_list:\r\n",
    "            rfr_tuning = RandomForestRegressor(n_estimators=160, max_features=max_features, min_samples_leaf=min_samples_leaf, bootstrap=True, criterion='squared_error', verbose=0, n_jobs=-1)\r\n",
    "            rfr_tuning.fit(X=X_tau_train_RF, y=Y_train_RF)\r\n",
    "            score = mean_squared_error(y_true=Y_val_RF, y_pred=rfr_tuning.predict(X_tau_val_RF))\r\n",
    "            if score < opt_score:\r\n",
    "                opt_param_RF = np.array([max_features,min_samples_leaf])\r\n",
    "                opt_score = score\r\n",
    "    \r\n",
    "    #definition and training of random forest regressor\r\n",
    "    rfr = RandomForestRegressor(n_estimators=400, criterion='squared_error', max_features=int(opt_param_RF[0]), min_samples_leaf=int(opt_param_RF[1]), bootstrap=True, verbose=0, warm_start=True, n_jobs=-1)\r\n",
    "    rfr.fit(X=X_tau_train_RF, y=Y_train_RF)\r\n",
    "    \r\n",
    "    #computation of the metrics (a), (b), (c) with B_1 and (c) with B_2 and training/valdiation MSE for the random forest\r\n",
    "    mse_train_RF = mean_squared_error(y_true=Y_train_RF, y_pred=rfr.predict(X_tau_train_RF))\r\n",
    "    mse_val_RF = mean_squared_error(y_true=Y_val_RF, y_pred=rfr.predict(X_tau_val_RF))\r\n",
    "    Y_pred_RF = rfr.predict(X_tau_test_RF)\r\n",
    "    mc_tmp = Y_pred_RF - Y_test_RF\r\n",
    "    metric_a_RF = np.sum(mc_tmp)/len(Y_test_RF)\r\n",
    "    metric_b_RF = np.sum((mc_tmp)*Y_pred_RF)/len(Y_test_RF)\r\n",
    "    metric_c_B_1_RF = np.sum(mc_tmp[B_1_RF])/len(Y_test_RF)\r\n",
    "    metric_c_B_2_RF = np.sum(mc_tmp[B_2_RF])/len(Y_test_RF)\r\n",
    "\r\n",
    "    #computation of the expected payoffs depending on the risk factor X_tau according to the models, i.e. computation of L_hat_i's\r\n",
    "    L_hat_RF = rfr.predict(X_tau_MC_RF)\r\n",
    "    L_hat_c_RF = np.column_stack((Z_MC_RF, L_hat_RF))\r\n",
    "    \r\n",
    "    #calculation of the IS estimators for Value-at-Risk and Expected Shortfall\r\n",
    "    L_hat_c_sort_RF = L_hat_c_RF[L_hat_c_RF[:,-1].argsort()[::-1]]\r\n",
    "    w = f(L_hat_c_sort_RF[:,0:3])/(M_MC*f_theta(x=IS_RF, y=L_hat_c_sort_RF[:,0:3]))\r\n",
    "\r\n",
    "    j_VaR = 0\r\n",
    "    w_sum_tmp = 0\r\n",
    "    while (w_sum_tmp <= (1-alpha_VaR) and j_VaR<M_MC):\r\n",
    "        w_sum_tmp += w[j_VaR]\r\n",
    "        j_VaR += 1\r\n",
    "    VaR_hat_RF = L_hat_c_sort_RF[j_VaR,-1]\r\n",
    "    print('VaR_hat_RF:',VaR_hat_RF)\r\n",
    "    \r\n",
    "    j_ES = 0\r\n",
    "    w_sum_tmp = 0\r\n",
    "    while (w_sum_tmp <= (1-alpha_ES) and j_ES<M_MC):\r\n",
    "        w_sum_tmp += w[j_ES]\r\n",
    "        j_ES += 1\r\n",
    "    ES_hat_RF = (1/(1-alpha_ES)) * np.sum(w[0:j_ES-1]*L_hat_c_sort_RF[0:j_ES-1,-1]) + ( 1 - (1 / (1-alpha_ES)) * np.sum(w[0:j_ES-1]) )*L_hat_c_sort_RF[j_ES,-1]\r\n",
    "    print('ES_hat_RF:',ES_hat_RF)\r\n",
    "    \r\n",
    "    #save results for further evaluation\r\n",
    "    output = np.array([[mse_train_NN,mse_val_NN,metric_a_NN,metric_b_NN,metric_c_B_1_NN,metric_c_B_2_NN,IS_NN[0],IS_NN[1],IS_NN[2],IS_NN[3],IS_NN[4],IS_NN[5],VaR_hat_NN,ES_hat_NN],\r\n",
    "                      [mse_train_RF,mse_val_RF,metric_a_RF,metric_b_RF,metric_c_B_1_RF,metric_c_B_2_RF,IS_RF[0],IS_RF[1],IS_RF[2],IS_RF[3],IS_RF[4],IS_RF[5],VaR_hat_RF,ES_hat_RF]])\r\n",
    "\r\n",
    "    joblib.dump(output,filepath+'output_'+str(run)+'_'+str(j)+'.joblib')\r\n",
    "    #prints just for checking while the notebook is running\r\n",
    "    print(j)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-11-04 16:17:42.503749: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-04 16:17:48.746563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30988 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n",
      "2021-11-04 16:17:48.828304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30988 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0\n",
      "2021-11-04 16:17:48.829844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30988 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.0\n",
      "2021-11-04 16:17:48.831335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30988 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0\n",
      "2021-11-04 16:17:52.453512: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "q_alpha_IS_hat_NN: 137.52938842773438\n",
      "q_alpha_IS_hat_RF: 138.5927021016384\n",
      "IS_NN: [ 2.42877391 -0.24341082 -1.25619823  0.41620164  1.00687489  1.22599225]\n",
      "IS_RF: [ 2.5983282  -0.12053818 -0.84588521  0.17220427  0.94157087  0.59334755]\n",
      "VaR_hat_NN: 140.20606994628906\n",
      "ES_hat_NN: 142.31917168960132\n",
      "VaR_hat_RF: 142.28552148299588\n",
      "ES_hat_RF: 143.47027066657185\n",
      "0\n",
      "q_alpha_IS_hat_NN: 136.5361328125\n",
      "q_alpha_IS_hat_RF: 138.4588826690527\n",
      "IS_NN: [ 2.44107828 -0.19289254 -1.15012123  0.53636081  1.03106079  1.55811656]\n",
      "IS_RF: [ 2.51800332 -0.08390248 -1.11060404  0.20290914  0.92000577  0.5162339 ]\n",
      "VaR_hat_NN: 142.19570922851562\n",
      "ES_hat_NN: 144.15243108224092\n",
      "VaR_hat_RF: 142.74116604976984\n",
      "ES_hat_RF: 143.7917491370757\n",
      "1\n",
      "q_alpha_IS_hat_NN: 138.4679412841797\n",
      "q_alpha_IS_hat_RF: 138.82728499803665\n",
      "IS_NN: [ 2.44643276 -0.33850762 -1.16920155  0.439363    0.98151032  1.41144212]\n",
      "IS_RF: [ 2.39697797 -0.21501542 -1.25457152  0.26745182  0.83937081  0.48587151]\n",
      "VaR_hat_NN: 140.89019775390625\n",
      "ES_hat_NN: 143.15976560201796\n",
      "VaR_hat_RF: 141.52066725610186\n",
      "ES_hat_RF: 143.4220728357287\n",
      "2\n",
      "q_alpha_IS_hat_NN: 136.7587127685547\n",
      "q_alpha_IS_hat_RF: 139.53005858868997\n",
      "IS_NN: [ 2.30819896 -0.38650343 -1.44704973  0.47532154  1.05311538  1.07138084]\n",
      "IS_RF: [ 2.55441228 -0.01209729 -1.04198622  0.19789067  0.94530104  0.59917731]\n",
      "VaR_hat_NN: 140.83029174804688\n",
      "ES_hat_NN: 142.90333035519663\n",
      "VaR_hat_RF: 143.20778420050678\n",
      "ES_hat_RF: 145.43872310356045\n",
      "3\n",
      "q_alpha_IS_hat_NN: 140.39700317382812\n",
      "q_alpha_IS_hat_RF: 139.77422981389935\n",
      "IS_NN: [ 2.45685758 -0.08706997 -1.22054341  0.38719411  1.07212828  1.265773  ]\n",
      "IS_RF: [ 2.60889041 -0.06827732 -0.91832333  0.21747638  0.92307989  0.5956423 ]\n",
      "VaR_hat_NN: 137.1903076171875\n",
      "ES_hat_NN: 139.2881818862929\n",
      "VaR_hat_RF: 141.03036257673796\n",
      "ES_hat_RF: 142.97221993412032\n",
      "4\n",
      "q_alpha_IS_hat_NN: 138.95428466796875\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/slurm_tmpdir/job_20152427/ipykernel_2422915/2204393149.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#Calculating the (hopefully) approximately optimal IS density parameters by minimising m_2 using the approximated g\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mIS_NN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_q_alpha_hat_reweighted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_hat_IS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_alpha_IS_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#define and train a random forest according to the optimal parameters from tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_lsq/least_squares.py\u001b[0m in \u001b[0;36mleast_squares\u001b[0;34m(fun, x0, jac, bounds, method, ftol, xtol, gtol, x_scale, loss, f_scale, diff_step, tr_solver, tr_options, jac_sparsity, max_nfev, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m         result = trf(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol, xtol,\n\u001b[0m\u001b[1;32m    929\u001b[0m                      \u001b[0mgtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_nfev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_solver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                      tr_options.copy(), verbose)\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_lsq/trf.py\u001b[0m in \u001b[0;36mtrf\u001b[0;34m(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale, loss_function, tr_solver, tr_options, verbose)\u001b[0m\n\u001b[1;32m    121\u001b[0m             loss_function, tr_solver, tr_options, verbose)\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         return trf_bounds(\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mftol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_nfev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             loss_function, tr_solver, tr_options, verbose)\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_lsq/trf.py\u001b[0m in \u001b[0;36mtrf_bounds\u001b[0;34m(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale, loss_function, tr_solver, tr_options, verbose)\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0mnjev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_lsq/least_squares.py\u001b[0m in \u001b[0;36mjac_wrapped\u001b[0;34m(x, f)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mjac_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m                 J = approx_derivative(fun, x, rel_step=diff_step, method=jac,\n\u001b[0m\u001b[1;32m    887\u001b[0m                                       \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                                       kwargs=kwargs, sparsity=jac_sparsity)\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparsity\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             return _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[0m\u001b[1;32m    487\u001b[0m                                      use_one_sided, method)\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Recompute dx as exactly representable number.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'3-point'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_one_sided\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             raise RuntimeError(\"`fun` return value has \"\n",
      "\u001b[0;32m/scratch/slurm_tmpdir/job_20152427/ipykernel_2422915/1160621729.py\u001b[0m in \u001b[0;36mg_q_alpha_hat_reweighted\u001b[0;34m(x, L, q_alpha_hat)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mg_q_alpha_hat_reweighted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_alpha_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mf_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mq_alpha_hat\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mbnds_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/slurm_tmpdir/job_20152427/ipykernel_2422915/1160621729.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mg_q_alpha_hat_reweighted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_alpha_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mf_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mq_alpha_hat\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mbnds_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/slurm_tmpdir/job_20152427/ipykernel_2422915/1160621729.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36mpdf\u001b[0;34m(self, x, mean, cov, allow_singular)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_quantiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mpsd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_PSD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_singular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_pdet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_squeeze_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# Note that eigh takes care of array conversion, chkfinite,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# and assertion that the matrix is square.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_eigvalsh_to_eps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/linalg/decomp.py\u001b[0m in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite, subset_by_index, subset_by_value, driver)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;31m# Multiple lwork vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0mlwork_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlwork_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpfx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mlwork_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'lwork'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}