{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Results were produced in stints. This is the number of the last stint.\r\n",
    "run = 11"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "from sklearn.utils import shuffle\r\n",
    "from scipy import stats\r\n",
    "from scipy import optimize\r\n",
    "import joblib\r\n",
    "\r\n",
    "#folder for saving results\r\n",
    "filepath = \".../Resultate_final/VarAnn/IS/VarAnn_GlueVaR_IS_sim_saved/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#parameters as in section 4.3 of 'Assessing Asset-Liability risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "#and section 5.1 of 'A Least-Squares Monte Carlo Approach to the Estimation of Enterprise Risk' (Ha, Bauer 2019)\r\n",
    "#parameters for q\r\n",
    "q_0 = 4.605\r\n",
    "m = 0.05\r\n",
    "sigma_S = 0.18\r\n",
    "#parameters for r\r\n",
    "r_0 = 0.025\r\n",
    "zeta = 0.25\r\n",
    "gamma = 0.02\r\n",
    "sigma_r = 0.01\r\n",
    "lambd = 0.02\r\n",
    "gamma_bar = gamma - (lambd*sigma_r)/zeta\r\n",
    "#parameters for mu_(55+t)\r\n",
    "mu_55 = 0.01\r\n",
    "kappa = 0.07\r\n",
    "sigma_mu = 0.0012\r\n",
    "#parameters for brownian motion\r\n",
    "rho_12 = -0.3\r\n",
    "rho_13 = 0.06\r\n",
    "rho_23 = -0.04\r\n",
    "cov_mat = np.array([[1,rho_12,rho_13],[rho_12,1,rho_23],[rho_13,rho_23,1]])\r\n",
    "#horizon parameters\r\n",
    "tau = 1\r\n",
    "T = 15\r\n",
    "b = 10.792\r\n",
    "#GlueVaR parameters\r\n",
    "alpha_Glue = 0.95\r\n",
    "beta_Glue = 0.995\r\n",
    "omega_Glue = np.array([1/3,1/3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Sizes for training, validation, test, and set size for Monte Carlo estimation of the risk measures\r\n",
    "M_1 = 1500000\r\n",
    "M_2 = 500000\r\n",
    "M_3 = 500000\r\n",
    "#ignore N or N_2 in the following. Was kept just in case, but not used.\r\n",
    "N_2 = 1\r\n",
    "M_MC = 500000\r\n",
    "#size of the set of data points used to calculate an IS density\r\n",
    "M_IS = 750000\r\n",
    "#quantile for which the IS density will be computed\r\n",
    "alpha_IS = 0.975"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Definition of these functions analogously to the appendix of 'A Least-Squares Monte Carlo Approach to the Estimation of Enterprise Risk' (Ha, Bauer 2019)\r\n",
    "def B_r(t,T):\r\n",
    "    return ((1-np.exp(-zeta*(T-t)))/zeta)\r\n",
    "def B_mu(t,T):\r\n",
    "    return ((np.exp(kappa*(T-t))-1)/kappa)\r\n",
    "def A(t,T):\r\n",
    "    tmp1 = gamma_bar*(B_r(t,T)-(T-t))\r\n",
    "    tmp2 = (sigma_r/zeta)**2 * ((T-t) - 2*B_r(t,T) + (1-np.exp(-2*zeta*(T-t)))/(2*zeta))\r\n",
    "    tmp3 = (sigma_mu/kappa)**2 * ((T-t) - 2*B_mu(t,T) + (np.exp(2*kappa*(T-t))-1)/(2*kappa))\r\n",
    "    tmp4 =  2*rho_23*sigma_r*sigma_mu/(zeta*kappa) * (B_mu(t,T) - (T-t) + B_r(t,T) - (1-np.exp(-(zeta-kappa)*(T-t)))/(zeta-kappa))\r\n",
    "    return np.exp(tmp1+0.5*(tmp2+tmp3+tmp4))\r\n",
    "#Definition of this function analogously to section 4.3 of 'Assessing Asset-Liability risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "def F(t,k,r_t,mu_xt):\r\n",
    "    return (A(t,t+k)*np.exp(-B_r(t,t+k)*r_t - B_mu(t,t+k)*mu_xt))\r\n",
    "\r\n",
    "#parameters of the normal distribution of X_tau according to the appendix of 'A Least-Squares Monte Carlo Approach to the Estimation of Enterprise Risk' (Ha, Bauer 2019)\r\n",
    "mu_q_tau = q_0 + (m-0.5*(sigma_S**2))*tau\r\n",
    "mu_r_tau = r_0*np.exp(-zeta*tau) + gamma*(1-np.exp(-zeta*tau))\r\n",
    "mu_mu_55_tau = mu_55*np.exp(kappa*tau)\r\n",
    "mean_tau = np.array([mu_q_tau, mu_r_tau, mu_mu_55_tau])\r\n",
    "\r\n",
    "cov_q_r_tau = rho_12*sigma_S*sigma_r*B_r(0,tau)\r\n",
    "cov_q_mu_tau = rho_13*sigma_S*sigma_mu*B_mu(0,tau)\r\n",
    "cov_r_mu_tau = rho_23*sigma_r*sigma_mu* ((1-np.exp(-(zeta-kappa)*tau))/(zeta-kappa))\r\n",
    "var_q_tau = (sigma_S**2) * tau\r\n",
    "var_r_tau = (sigma_r**2) * ((1-np.exp(-2*zeta*tau))/(2*zeta))\r\n",
    "var_mu_tau = (sigma_mu**2) * ((np.exp(2*kappa*tau)-1)/(2*kappa))\r\n",
    "cov_mat_tau = np.array([[var_q_tau, cov_q_r_tau, cov_q_mu_tau], [cov_q_r_tau, var_r_tau, cov_r_mu_tau], [cov_q_mu_tau, cov_r_mu_tau, var_mu_tau]])\r\n",
    "C_tau = np.linalg.cholesky(cov_mat_tau)\r\n",
    "\r\n",
    "#variance/covariance parameters of the conditional normal distribution of X_T according to the appendix of 'A Least-Squares Monte Carlo Approach to the Estimation of Enterprise Risk' (Ha, Bauer 2019)\r\n",
    "var_q_T_cond = (sigma_S**2) *(T-tau) + ((sigma_r/zeta)**2) * (T-tau - 2*(1-np.exp(-zeta*(T-tau)))/zeta + (1-np.exp(-2*zeta*(T-tau)))/(2*zeta)) + (2*rho_12*sigma_S*sigma_r/zeta) * (T-tau- (1-np.exp(-zeta*(T-tau)))/zeta)\r\n",
    "cov_q_r_T_cond = rho_12*sigma_S*sigma_r*((1-np.exp(-zeta*(T-tau)))/zeta) + ((sigma_r**2)/zeta) * ((1-2*np.exp(-zeta*(T-tau))+np.exp(-2*zeta*(T-tau)))/(2*zeta))\r\n",
    "cov_q_mu_T_cond = rho_13*sigma_S*sigma_mu *((np.exp(kappa*(T-tau))-1)/kappa) + (rho_23*sigma_r*sigma_mu/zeta) * ((np.exp(kappa*(T-tau))-1)/kappa - (1-np.exp(-(zeta-kappa)*(T-tau)))/(zeta-kappa))\r\n",
    "var_r_T_cond = (sigma_r**2) * ((1-np.exp(-2*zeta*(T-tau)))/(2*zeta))\r\n",
    "cov_r_mu_T_cond = rho_23*sigma_r*sigma_mu*((1-np.exp(-(zeta-kappa)*(T-tau)))/(zeta-kappa))\r\n",
    "var_mu_T_cond = (sigma_mu**2) * ((np.exp(2*kappa*(T-tau))-1)/(2*kappa))\r\n",
    "cov_mat_T_cond = np.array([[var_q_T_cond, cov_q_r_T_cond, cov_q_mu_T_cond], [cov_q_r_T_cond, var_r_T_cond, cov_r_mu_T_cond], [cov_q_mu_T_cond, cov_r_mu_T_cond, var_mu_T_cond]])\r\n",
    "\r\n",
    "#function for generating simulated risk factors X_tau and corresponding payments Y from multivariate standard normal random variables\r\n",
    "def data_gen(Z,V):\r\n",
    "    #simulation of X_tau\r\n",
    "    X_tau = np.transpose(np.matmul(C_tau,np.transpose(Z))) + np.tile(mean_tau, (len(Z),1))\r\n",
    "\r\n",
    "    #simulation of X_T given X_tau\r\n",
    "    mu_q_T_cond = X_tau[:,0] + B_r(tau,T)*X_tau[:,1] + (gamma_bar - (sigma_r/zeta)**2)*(T-tau - (1-np.exp(-zeta*(T-tau)))/zeta) + 0.5*((sigma_r/zeta)**2) * ((1-np.exp(-zeta*(T-tau)))/zeta - (np.exp(-zeta*(T-tau))-np.exp(-2*zeta*(T-tau)))/zeta) - ((rho_23*sigma_r*sigma_mu)/kappa) * ( (np.exp(kappa*(T-tau))-1)/(kappa*(zeta-kappa)) - (np.exp(kappa*(T-tau))-np.exp(-(zeta-kappa)*(T-tau)))/(zeta*(zeta-kappa)) - (1/zeta) * (T-tau - (1-np.exp(-zeta*(T-tau)))/zeta)) -0.5*(sigma_S**2) * (T-tau) - (rho_12*sigma_S*sigma_r/zeta) * (T-tau - (1-np.exp(-zeta*(T-tau)))/zeta) - (rho_13*sigma_S*sigma_mu/kappa) * ((np.exp(kappa*(T-tau))-1)/kappa -T+tau)\r\n",
    "    mu_r_T_cond = np.exp(-zeta*(T-tau))*X_tau[:,1] + (gamma_bar-(sigma_r/zeta)**2)*(1-np.exp(-zeta*(T-tau))) + 0.5*((sigma_r/zeta)**2) *(1-np.exp(-2*zeta*(T-tau))) - (rho_23*sigma_r*sigma_mu/kappa) * ((1-np.exp(-(zeta-kappa)*(T-tau)))/(zeta-kappa) - (1-np.exp(-zeta*(T-tau)))/zeta)\r\n",
    "    mu_mu_T_cond = np.exp(kappa*(T-tau))*X_tau[:,2] - (rho_23*sigma_r*sigma_mu/zeta) * ((np.exp(kappa*(T-tau))-1)/kappa - (1-np.exp(-(zeta-kappa)*(T-tau)))/(zeta-kappa)) - ((sigma_mu**2)/kappa) * ((np.exp(2*kappa*(T-tau))-1)/(2*kappa) - (np.exp(kappa*(T-tau))-1)/kappa)\r\n",
    "    mean_T_cond = np.array([mu_q_T_cond, mu_r_T_cond, mu_mu_T_cond])\r\n",
    "\r\n",
    "    X_T = V + np.transpose(mean_T_cond)\r\n",
    "    \r\n",
    "    #calculation of Y from X_T and X_tau\r\n",
    "    Y = F(t=tau, k=T-tau, r_t=X_tau[:,1], mu_xt=X_tau[:,2]) * np.maximum(np.exp(X_T[:,0]), b*np.sum([F(t=T, k=i, r_t=X_T[:,1], mu_xt=X_T[:,2]) for i in range(1,51)], axis=0))\r\n",
    "    return X_tau, Y\r\n",
    "\r\n",
    "#the function DT(Z,\\theta)\r\n",
    "def data_trans_IS(Z,IS):\r\n",
    "    res = np.empty((len(Z),3))\r\n",
    "    for j in range(3):\r\n",
    "        res[:,j] = Z[:,j]*np.sqrt(IS[3+j]) + IS[j]\r\n",
    "    return res\r\n",
    "\r\n",
    "#The density function of Z\r\n",
    "def f(y):\r\n",
    "    return stats.multivariate_normal.pdf(y, mean=np.full(3,0), cov=np.identity(3))\r\n",
    "\r\n",
    "#The density function of Z_\\theta (note that x is interpreted as theta, needed for the least-squares solver to work properly)\r\n",
    "def f_theta(y, x):\r\n",
    "    return stats.multivariate_normal.pdf(y, mean=x[0:3], cov=np.diag(x[3:6]))\r\n",
    "\r\n",
    "#This function describes the approximation of the expression inside the sum of m_2(theta)\r\n",
    "def g_q_alpha_hat_reweighted(x,L,q_alpha_hat):\r\n",
    "    return [(np.sqrt(f(y=L[i,0:3])/f_theta(x=x, y=L[i,0:3])) if L[i,-1]>q_alpha_hat else 0) for i in range(len(L))]\r\n",
    "\r\n",
    "#bounds for the IS density parameters (for the parameters corresponding to the mean no bounds are necessary, the standard deviation parameters however needs to be non-negative)\r\n",
    "bnds_lower = np.array([-np.inf, -np.inf, -np.inf, 0,0,0])\r\n",
    "bnds_upper = np.full(6,np.inf)\r\n",
    "bnds = (bnds_lower, bnds_upper)\r\n",
    "\r\n",
    "#function for calculating GlueVaR in an IS setting\r\n",
    "def GlueVaR_IS(omega, L, alpha, beta, w):\r\n",
    "    j_beta = 0\r\n",
    "    w_sum_tmp = 0\r\n",
    "    while(w_sum_tmp <= (1-beta)):\r\n",
    "        w_sum_tmp += w[j_beta]\r\n",
    "        j_beta += 1\r\n",
    "        \r\n",
    "    j_alpha = j_beta\r\n",
    "    while(w_sum_tmp <= (1-alpha)):\r\n",
    "        w_sum_tmp += w[j_alpha]\r\n",
    "        j_alpha += 1\r\n",
    "        \r\n",
    "    ES_beta = 1/(1-beta) * np.sum(w[0:j_beta-1]*L[0:j_beta-1]) + ( 1 - (1 / (1-beta)) * np.sum(w[0:j_beta-1]) )*L[j_beta]\r\n",
    "    ES_alpha = 1/(1-alpha) * np.sum(w[0:j_alpha-1]*L[0:j_alpha-1]) + ( 1 - (1 / (1-alpha)) * np.sum(w[0:j_alpha-1]) )*L[j_alpha]\r\n",
    "    VaR_alpha = L[j_alpha]\r\n",
    "\r\n",
    "    return omega[0]*ES_beta + omega[1]*ES_alpha + (1-omega[0]-omega[1])*VaR_alpha"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for j in range(100):\r\n",
    "    #Generating realisations of multivariate standard normal random variables, V correlated\r\n",
    "    Z_IS = np.random.multivariate_normal(mean=np.full(3,0), cov=np.identity(3), size=M_IS)\r\n",
    "    V_IS = np.random.multivariate_normal(mean=np.full(3,0), cov=cov_mat_T_cond, size=M_IS)\r\n",
    "\r\n",
    "    #Calculate the risk factor X_tau and the corresponding simulated payoffs Y\r\n",
    "    X_tau_IS, Y_IS = data_gen(Z=Z_IS, V=V_IS)\r\n",
    "\r\n",
    "    #define and compile neural network model, setup as in section 4.3 of 'Assessing Asset-Liability Risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "    bi_IS = np.log( np.sum(Y_IS)/len(Y_IS))\r\n",
    "    model_IS = tf.keras.models.Sequential([\r\n",
    "        tf.keras.layers.BatchNormalization(input_shape=(3,)),\r\n",
    "        tf.keras.layers.Dense(4, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(4, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(1, activation='exponential', bias_initializer=tf.keras.initializers.Constant(value=bi_IS))])\r\n",
    "    model_IS.compile(loss='mse', optimizer='adam', metrics=['mse'])\r\n",
    "    model_IS.fit(x=X_tau_IS, y=Y_IS, epochs=40, batch_size=10000, verbose=0)\r\n",
    "\r\n",
    "    #Calculate realisations of L_hat from the training data set using the trained neural network\r\n",
    "    L_hat_IS = np.column_stack((Z_IS, model_IS.predict(X_tau_IS)[:,0]))\r\n",
    "    L_hat_IS_sort = L_hat_IS[L_hat_IS[:,-1].argsort()[::-1]]\r\n",
    "\r\n",
    "    #Calculating the corresponding estimator for Value-at-Risk in order to approximate g\r\n",
    "    q_alpha_IS_hat = L_hat_IS_sort[int(M_IS*(1-alpha_IS)-1), -1]\r\n",
    "    print('q_alpha_IS_hat_NN:',q_alpha_IS_hat)\r\n",
    "\r\n",
    "    #Calculating the (hopefully) approximately optimal \\theta^*_{NN} by minimising m_2 using the approximated g\r\n",
    "    IS_NN = optimize.least_squares(g_q_alpha_hat_reweighted, x0=np.concatenate((np.full(3,0),np.full(3,1))), args=(L_hat_IS, q_alpha_IS_hat), bounds=bnds).x\r\n",
    "\r\n",
    "    #define and train a random forest according to the optimal parameters from tuning\r\n",
    "    rfr_IS = RandomForestRegressor(n_estimators=160, criterion='squared_error', max_features=2, min_samples_leaf=1300, bootstrap=True, verbose=0, n_jobs=-1)\r\n",
    "    rfr_IS.fit(X=X_tau_IS, y=Y_IS)\r\n",
    "\r\n",
    "    #Calculate realisations of L_hat from the training data set using the trained random forest\r\n",
    "    L_hat_IS = np.column_stack((Z_IS, rfr_IS.predict(X_tau_IS)))\r\n",
    "    L_hat_IS_sort = L_hat_IS[L_hat_IS[:,-1].argsort()[::-1]]\r\n",
    "\r\n",
    "    #Calculating the corresponding estimator for Value-at-Risk in order to approximate g\r\n",
    "    q_alpha_IS_hat = L_hat_IS_sort[int(M_IS*(1-alpha_IS)-1), -1]\r\n",
    "    print('q_alpha_IS_hat_RF:',q_alpha_IS_hat)\r\n",
    "\r\n",
    "    #Calculating the (hopefully) approximately optimal \\theta^*_{RF} by minimising m_2 using the approximated g\r\n",
    "    IS_RF = optimize.least_squares(g_q_alpha_hat_reweighted, x0=np.concatenate((np.full(3,0),np.full(3,1))), args=(L_hat_IS, q_alpha_IS_hat), bounds=bnds).x\r\n",
    "\r\n",
    "    #print IS density parameters for checking\r\n",
    "    print('IS_NN:',IS_NN)\r\n",
    "    print('IS_RF:',IS_RF)\r\n",
    "    \r\n",
    "    #Generating simulations for multivariate standard normal random variables (uncorrelated for Z and correlated for V) for training set, validation set, test set, set for Monte Carlo estimation of risk measures\r\n",
    "    Z_train = np.random.multivariate_normal(mean=np.full(3,0), cov=np.identity(3), size=M_1)\r\n",
    "    V_train = np.random.multivariate_normal(mean=np.full(3,0), cov=cov_mat_T_cond, size=M_1)\r\n",
    "    Z_val = np.random.multivariate_normal(mean=np.full(3,0), cov=np.identity(3), size=M_2)\r\n",
    "    V_val = np.random.multivariate_normal(mean=np.full(3,0), cov=cov_mat_T_cond, size=M_2)\r\n",
    "    Z_test = np.random.multivariate_normal(mean=np.full(3,0), cov=np.identity(3), size=M_3)\r\n",
    "    V_test = np.random.multivariate_normal(mean=np.full(3,0), cov=cov_mat_T_cond, size=M_3)\r\n",
    "    Z_MC = np.random.multivariate_normal(mean=np.full(3,0), cov=np.identity(3), size=M_MC)\r\n",
    "    V_MC = np.random.multivariate_normal(mean=np.full(3,0), cov=cov_mat_T_cond, size=M_MC)\r\n",
    "\r\n",
    "    #calculate DT(Z,\\theta^*_{NN})\r\n",
    "    Z_train_NN = data_trans_IS(Z_train,IS_NN)\r\n",
    "    Z_val_NN = data_trans_IS(Z_val,IS_NN)\r\n",
    "    Z_test_NN = data_trans_IS(Z_test,IS_NN)\r\n",
    "    Z_MC_NN = data_trans_IS(Z_MC,IS_NN)\r\n",
    "    #calculating the risk factors under the IS distribution and corresponding realised payoffs\r\n",
    "    X_tau_train_NN, Y_train_NN = data_gen(Z=Z_train_NN, V=V_train)\r\n",
    "    X_tau_val_NN, Y_val_NN = data_gen(Z=Z_val_NN, V=V_val)\r\n",
    "    X_tau_test_NN, Y_test_NN = data_gen(Z=Z_test_NN, V=V_test)\r\n",
    "    X_tau_MC_NN, Y_MC_NN = data_gen(Z=Z_MC_NN, V=V_MC)\r\n",
    "\r\n",
    "    #calculate DT(Z,\\theta^*_{RF})\r\n",
    "    Z_train_RF = data_trans_IS(Z_train,IS_RF)\r\n",
    "    Z_val_RF = data_trans_IS(Z_val,IS_RF)\r\n",
    "    Z_test_RF = data_trans_IS(Z_test,IS_RF)\r\n",
    "    Z_MC_RF = data_trans_IS(Z_MC,IS_RF)\r\n",
    "    #calculating the risk factors under the IS distribution and corresponding realised payoffs\r\n",
    "    X_tau_train_RF, Y_train_RF = data_gen(Z=Z_train_RF, V=V_train)\r\n",
    "    X_tau_val_RF, Y_val_RF = data_gen(Z=Z_val_RF, V=V_val)\r\n",
    "    X_tau_test_RF, Y_test_RF = data_gen(Z=Z_test_RF, V=V_test)\r\n",
    "    X_tau_MC_RF, Y_MC_RF = data_gen(Z=Z_MC_RF, V=V_MC)\r\n",
    "    \r\n",
    "    #calculating parameters for the sets B_1 and B_2\r\n",
    "    q_70 = stats.norm.ppf(0.7, loc=mu_q_tau, scale=var_q_tau)\r\n",
    "    q_30 = stats.norm.ppf(0.3, loc=mu_q_tau, scale=var_q_tau)\r\n",
    "    r_70 = stats.norm.ppf(0.7, loc=mu_r_tau, scale=var_r_tau)\r\n",
    "    r_30 = stats.norm.ppf(0.3, loc=mu_r_tau, scale=var_r_tau)\r\n",
    "\r\n",
    "    #calculate the indices of the set B_1 and B_2 for the test set created with the IS density calculated by the neural network\r\n",
    "    B_1_NN = np.apply_along_axis(np.all, axis=1, arr=np.column_stack( (X_tau_test_NN[:,0] > q_70, X_tau_test_NN[:,1] < r_30)) )\r\n",
    "    B_2_NN = np.apply_along_axis(np.all, axis=1, arr=np.column_stack( (X_tau_test_NN[:,0] < q_30, X_tau_test_NN[:,1] > r_70)) )\r\n",
    "\r\n",
    "    #calculate the indices of the set B_1 and B_2 for the test set created with the IS density calculated by the random forest\r\n",
    "    B_1_RF = np.apply_along_axis(np.all, axis=1, arr=np.column_stack( (X_tau_test_RF[:,0] > q_70, X_tau_test_RF[:,1] < r_30)) )\r\n",
    "    B_2_RF = np.apply_along_axis(np.all, axis=1, arr=np.column_stack( (X_tau_test_RF[:,0] < q_30, X_tau_test_RF[:,1] > r_70)) )\r\n",
    "    \r\n",
    "    #define and compile neural network model, setup as in section 4.3 of 'Assessing Asset-Liability Risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "    bi = np.log( np.sum(Y_train_NN)/len(Y_train_NN))\r\n",
    "    model = tf.keras.models.Sequential([\r\n",
    "        tf.keras.layers.BatchNormalization(input_shape=(3,)),\r\n",
    "        tf.keras.layers.Dense(4, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(4, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(1, activation='exponential', bias_initializer=tf.keras.initializers.Constant(value=bi))])\r\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\r\n",
    "    #training the neural network\r\n",
    "    hist = model.fit(x=X_tau_train_NN, y=Y_train_NN, epochs=40, batch_size=10000, validation_data=(X_tau_val_NN,Y_val_NN), verbose=0)\r\n",
    "    \r\n",
    "    #computation of the metrics (a), (b), (c) with B_1 and (c) with B_2 for the neural network\r\n",
    "    Y_pred_NN = model.predict(X_tau_test_NN)[:,0]\r\n",
    "    mse_train_NN = hist.history['mse'][-1]\r\n",
    "    mse_val_NN = hist.history['val_mse'][-1]\r\n",
    "    mc_tmp = Y_pred_NN - Y_test_NN\r\n",
    "    metric_a_NN = np.sum(mc_tmp)/len(Y_test_NN)\r\n",
    "    metric_b_NN = np.sum((mc_tmp)*Y_pred_NN)/len(Y_test_NN)\r\n",
    "    metric_c_B_1_NN = np.sum(mc_tmp[B_1_NN])/len(Y_test_NN)\r\n",
    "    metric_c_B_2_NN = np.sum(mc_tmp[B_2_NN])/len(Y_test_NN)\r\n",
    "\r\n",
    "    #computation of expected payoffs depending on the risk factor X_tau according to the models, i.e. computation of L_hat_i's\r\n",
    "    L_hat_NN = model.predict(X_tau_MC_NN)[:,0]\r\n",
    "    L_hat_c_NN = np.column_stack((Z_MC_NN, L_hat_NN))\r\n",
    "    \r\n",
    "    #calculation of the IS estimator for GlueVaR\r\n",
    "    L_hat_c_sort_NN = L_hat_c_NN[L_hat_c_NN[:,-1].argsort()[::-1]]\r\n",
    "    w = f(L_hat_c_sort_NN[:,0:3])/(M_MC*f_theta(x=IS_NN, y=L_hat_c_sort_NN[:,0:3]))\r\n",
    "\r\n",
    "    GlueVaR_hat_NN = GlueVaR_IS(omega=omega_Glue, L=L_hat_c_sort_NN[:,-1], alpha=alpha_Glue, beta=beta_Glue, w=w)\r\n",
    "    print('GlueVaR_hat_NN:',GlueVaR_hat_NN)\r\n",
    "    \r\n",
    "    \r\n",
    "    #perform a grid search in order to find the (approximately) best hyperparameter min_samples_leaf\r\n",
    "    #values that will be checked\r\n",
    "    max_features_list = [2]\r\n",
    "    min_samples_leaf_list = [2250,2500,2750]\r\n",
    "    opt_param = np.full(2,0)\r\n",
    "    opt_score = np.inf\r\n",
    "\r\n",
    "    for max_features in max_features_list:\r\n",
    "        for min_samples_leaf in min_samples_leaf_list:\r\n",
    "            rfr_tuning = RandomForestRegressor(n_estimators=160, max_features=max_features, min_samples_leaf=min_samples_leaf, bootstrap=True, criterion='squared_error', verbose=0, n_jobs=-1)\r\n",
    "            rfr_tuning.fit(X=X_tau_train_RF, y=Y_train_RF)\r\n",
    "            score = mean_squared_error(y_true=Y_val_RF, y_pred=rfr_tuning.predict(X_tau_val_RF))\r\n",
    "            if score < opt_score:\r\n",
    "                opt_param_RF = np.array([max_features,min_samples_leaf])\r\n",
    "                opt_score = score\r\n",
    "    \r\n",
    "    #definition and training of random forest regressor\r\n",
    "    rfr = RandomForestRegressor(n_estimators=400, criterion='squared_error', max_features=int(opt_param_RF[0]), min_samples_leaf=int(opt_param_RF[1]), bootstrap=True, verbose=0, warm_start=True, n_jobs=-1)\r\n",
    "    rfr.fit(X=X_tau_train_RF, y=Y_train_RF)\r\n",
    "    \r\n",
    "    #computation of the metrics (a), (b), (c) with B_1 and (c) with B_2 and training/valdiation MSE for the random forest\r\n",
    "    mse_train_RF = mean_squared_error(y_true=Y_train_RF, y_pred=rfr.predict(X_tau_train_RF))\r\n",
    "    mse_val_RF = mean_squared_error(y_true=Y_val_RF, y_pred=rfr.predict(X_tau_val_RF))\r\n",
    "    Y_pred_RF = rfr.predict(X_tau_test_RF)\r\n",
    "    mc_tmp = Y_pred_RF - Y_test_RF\r\n",
    "    metric_a_RF = np.sum(mc_tmp)/len(Y_test_RF)\r\n",
    "    metric_b_RF = np.sum((mc_tmp)*Y_pred_RF)/len(Y_test_RF)\r\n",
    "    metric_c_B_1_RF = np.sum(mc_tmp[B_1_RF])/len(Y_test_RF)\r\n",
    "    metric_c_B_2_RF = np.sum(mc_tmp[B_2_RF])/len(Y_test_RF)\r\n",
    "\r\n",
    "    #computation of the expected payoff depending on the risk factor X_tau according to the models, i.e. computation of L_hat_i's\r\n",
    "    L_hat_RF = rfr.predict(X_tau_MC_RF)\r\n",
    "    L_hat_c_RF = np.column_stack((Z_MC_RF, L_hat_RF))\r\n",
    "    \r\n",
    "    #calculation of the IS estimator for GlueVaR\r\n",
    "    L_hat_c_sort_RF = L_hat_c_RF[L_hat_c_RF[:,-1].argsort()[::-1]]\r\n",
    "    w = f(L_hat_c_sort_RF[:,0:3])/(M_MC*f_theta(x=IS_RF, y=L_hat_c_sort_RF[:,0:3]))\r\n",
    "\r\n",
    "    GlueVaR_hat_RF = GlueVaR_IS(omega=omega_Glue, L=L_hat_c_sort_RF[:,-1], alpha=alpha_Glue, beta=beta_Glue, w=w)\r\n",
    "    print('GlueVaR_hat_RF:',GlueVaR_hat_RF)\r\n",
    "    \r\n",
    "    #save results for further evaluation\r\n",
    "    output = np.array([[mse_train_NN,mse_val_NN,metric_a_NN,metric_b_NN,metric_c_B_1_NN,metric_c_B_2_NN,IS_NN[0],IS_NN[1],IS_NN[2],IS_NN[3],IS_NN[4],IS_NN[5],GlueVaR_hat_NN],\r\n",
    "                      [mse_train_RF,mse_val_RF,metric_a_RF,metric_b_RF,metric_c_B_1_RF,metric_c_B_2_RF,IS_RF[0],IS_RF[1],IS_RF[2],IS_RF[3],IS_RF[4],IS_RF[5],GlueVaR_hat_RF]])\r\n",
    "\r\n",
    "    joblib.dump(output,filepath+'output_'+str(run)+'_'+str(j)+'.joblib')\r\n",
    "    #prints just for checking while the notebook is running\r\n",
    "    print(j)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-11-06 14:19:45.922827: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-06 14:19:54.220195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30988 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:16:00.0, compute capability: 7.0\n",
      "2021-11-06 14:19:54.344716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30988 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3a:00.0, compute capability: 7.0\n",
      "2021-11-06 14:19:54.346262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30988 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2021-11-06 14:19:54.347794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30988 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n",
      "2021-11-06 14:19:57.503417: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "q_alpha_IS_hat_NN: 129.76986694335938\n",
      "q_alpha_IS_hat_RF: 129.19310908430504\n",
      "IS_NN: [ 1.98574138 -0.14414745 -0.93999126  0.39113847  0.9794789   1.10746994]\n",
      "IS_RF: [ 1.89814568 -0.15282591 -0.9265567   0.45136758  1.00446446  1.24918008]\n",
      "GlueVaR_hat_NN: 133.3975610364167\n",
      "GlueVaR_hat_RF: 134.93183007731025\n",
      "0\n",
      "q_alpha_IS_hat_NN: 128.48057556152344\n",
      "q_alpha_IS_hat_RF: 129.45409256818772\n",
      "IS_NN: [ 1.91643443 -0.1826985  -1.06361598  0.42924211  0.89690288  1.06219126]\n",
      "IS_RF: [ 2.055648   -0.09424235 -0.77423162  0.30643986  0.95705446  1.03333483]\n",
      "GlueVaR_hat_NN: 134.39774302907455\n",
      "GlueVaR_hat_RF: 134.58893066204763\n",
      "1\n",
      "q_alpha_IS_hat_NN: 129.32962036132812\n",
      "q_alpha_IS_hat_RF: 128.96330047229546\n",
      "IS_NN: [ 1.90998738 -0.23890163 -1.04667255  0.41240773  0.93050306  1.06916966]\n",
      "IS_RF: [ 1.90905244 -0.14097516 -0.92525491  0.42075976  1.0614694   1.33765117]\n",
      "GlueVaR_hat_NN: 133.48155894847872\n",
      "GlueVaR_hat_RF: 134.44858742300084\n",
      "2\n",
      "q_alpha_IS_hat_NN: 126.6164779663086\n",
      "q_alpha_IS_hat_RF: 129.31628505902245\n",
      "IS_NN: [ 1.61009376 -0.52058336 -1.31050262  0.51200133  0.830115    0.99388307]\n",
      "IS_RF: [ 1.96368201 -0.12502704 -0.88042589  0.3800075   0.9630288   1.24871913]\n",
      "GlueVaR_hat_NN: 135.8109467934334\n",
      "GlueVaR_hat_RF: 135.0590061596568\n",
      "3\n",
      "q_alpha_IS_hat_NN: 128.4287567138672\n",
      "q_alpha_IS_hat_RF: 129.10893908844537\n",
      "IS_NN: [ 1.91496862 -0.18389139 -1.00363741  0.48120511  0.96086561  1.27585054]\n",
      "IS_RF: [ 1.93394824 -0.09997958 -0.93920916  0.41718478  1.00415788  1.29243217]\n",
      "GlueVaR_hat_NN: 136.99132437588457\n",
      "GlueVaR_hat_RF: 134.7248191142438\n",
      "4\n",
      "q_alpha_IS_hat_NN: 128.71759033203125\n",
      "q_alpha_IS_hat_RF: 129.36787413765023\n",
      "IS_NN: [ 1.90109542 -0.24657653 -1.04425194  0.44937638  0.91848294  1.19504838]\n",
      "IS_RF: [ 1.94855052 -0.15215411 -0.90008596  0.3884665   0.94791001  1.2795379 ]\n",
      "GlueVaR_hat_NN: 133.6650113058953\n",
      "GlueVaR_hat_RF: 134.6978206985071\n",
      "5\n",
      "q_alpha_IS_hat_NN: 128.91064453125\n",
      "q_alpha_IS_hat_RF: 129.8149587114045\n",
      "IS_NN: [ 1.9304561  -0.2521453  -1.00012321  0.42327115  0.95820279  1.2303636 ]\n",
      "IS_RF: [ 1.8238476  -0.07945088 -1.06127093  0.56808633  0.97907836  1.40005731]\n",
      "GlueVaR_hat_NN: 133.79146818551123\n",
      "GlueVaR_hat_RF: 134.4364638957687\n",
      "6\n",
      "q_alpha_IS_hat_NN: 130.01707458496094\n",
      "q_alpha_IS_hat_RF: 129.5202666887367\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/slurm_tmpdir/job_20158915/ipykernel_157951/513450466.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m#Calculating the (hopefully) approximately optimal IS density parameters by minimising m_2 using the approximated g\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mIS_RF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_q_alpha_hat_reweighted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_hat_IS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_alpha_IS_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#print IS density parameters for checking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_lsq/least_squares.py\u001b[0m in \u001b[0;36mleast_squares\u001b[0;34m(fun, x0, jac, bounds, method, ftol, xtol, gtol, x_scale, loss, f_scale, diff_step, tr_solver, tr_options, jac_sparsity, max_nfev, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m         result = trf(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol, xtol,\n\u001b[0m\u001b[1;32m    929\u001b[0m                      \u001b[0mgtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_nfev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_solver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                      tr_options.copy(), verbose)\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_lsq/trf.py\u001b[0m in \u001b[0;36mtrf\u001b[0;34m(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale, loss_function, tr_solver, tr_options, verbose)\u001b[0m\n\u001b[1;32m    121\u001b[0m             loss_function, tr_solver, tr_options, verbose)\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         return trf_bounds(\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mftol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_nfev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             loss_function, tr_solver, tr_options, verbose)\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_lsq/trf.py\u001b[0m in \u001b[0;36mtrf_bounds\u001b[0;34m(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale, loss_function, tr_solver, tr_options, verbose)\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0mnjev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_lsq/least_squares.py\u001b[0m in \u001b[0;36mjac_wrapped\u001b[0;34m(x, f)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mjac_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m                 J = approx_derivative(fun, x, rel_step=diff_step, method=jac,\n\u001b[0m\u001b[1;32m    887\u001b[0m                                       \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                                       kwargs=kwargs, sparsity=jac_sparsity)\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparsity\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             return _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[0m\u001b[1;32m    487\u001b[0m                                      use_one_sided, method)\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Recompute dx as exactly representable number.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'3-point'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_one_sided\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             raise RuntimeError(\"`fun` return value has \"\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}