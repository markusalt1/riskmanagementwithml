{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "from sklearn.utils import shuffle\r\n",
    "from scipy import stats\r\n",
    "from scipy import optimize\r\n",
    "import joblib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "#directory for results\r\n",
    "filepath = \".../Resultate_final/PTF/IS/IS_test/PTF_VaR_ES_IS_1_test_saved/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Market and option parameters as in section 4.2 of 'Assessing Asset-Liability Risk with Neural Networks' (Cheridito, Ery, WÃ¼thrich 2020)\r\n",
    "s_0 = 100\r\n",
    "r = 0.01\r\n",
    "corr= 0.3\r\n",
    "tau = 1/52\r\n",
    "T = 1/3\r\n",
    "K = 100\r\n",
    "\r\n",
    "mu = np.empty(20)\r\n",
    "sigma = np.empty(20)\r\n",
    "for i in range(0,10):\r\n",
    "    mu[i] = mu[i+10] = (3+(i+1)/2)/100\r\n",
    "    sigma[i] = sigma[i+10] = (15+(i+1))/100\r\n",
    "\r\n",
    "cov_mat = np.empty((20,20))\r\n",
    "for i in range(0,20):\r\n",
    "    for j in range(0,20):\r\n",
    "        if i != j:\r\n",
    "            cov_mat[i,j] = corr\r\n",
    "        else:\r\n",
    "            cov_mat[i,j] = 1\r\n",
    "\r\n",
    "C = np.linalg.cholesky(cov_mat)\r\n",
    "\r\n",
    "#Confidence levels for Value-at-Risk and Expected Shortfall\r\n",
    "alpha_VaR = 0.995\r\n",
    "alpha_ES = 0.99"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#Sizes for training set, validation set, test set, and set size for Monte Carlo estimation of the risk measures\r\n",
    "M_1 = 1500000\r\n",
    "M_2 = 500000\r\n",
    "M_3 = 500000\r\n",
    "#ignore N or N_2 in the following. Was kept just in case, but not used.\r\n",
    "N_2 = 1\r\n",
    "M_MC = 500000\r\n",
    "#size of the set of data points used to calculate an IS density\r\n",
    "M_IS = 2000000\r\n",
    "#quantile for which the IS density will be computed\r\n",
    "alpha_IS = 0.995"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Function for calculating simulated values of S_tau and simulated payoffs P_T from simulations of multivariate standard normal random variables\r\n",
    "def data_gen(M,N,Z,V):\r\n",
    "    #correlating the independent components\r\n",
    "    Z = np.transpose(np.matmul(C,np.transpose(Z)))\r\n",
    "    V = np.transpose(np.matmul(C,np.transpose(V)))\r\n",
    "    \r\n",
    "    #simulate S_tau under P\r\n",
    "    S_tau_pre = np.empty((M, 20))\r\n",
    "    for j in range(0,20):\r\n",
    "        S_tau_pre[:,j] = s_0 * np.exp( (mu[j]-0.5*sigma[j]**2)*tau + np.sqrt(tau)*sigma[j]*Z[:,j] )\r\n",
    "    S_tau = np.tile(S_tau_pre, (N,1))\r\n",
    "\r\n",
    "    #simulate S_T given S_tau under Q\r\n",
    "    S_T = np.empty((N*M,20))\r\n",
    "    for j in range(0,20):\r\n",
    "        S_T[:,j] = S_tau[:,j] * np.exp( (r-0.5*sigma[j]**2)*(T-tau) + np.sqrt(T-tau)*sigma[j]*V[:,j] )\r\n",
    "\r\n",
    "    #compute discounted option payoffs\r\n",
    "    P_T_pre =np.empty((len(S_T), 20))\r\n",
    "    for j in range(0,10):\r\n",
    "        P_T_pre[:,j] = np.exp(-r*(T-tau)) * np.maximum(S_T[:,j]-K,0)\r\n",
    "    for j in range(10,20):\r\n",
    "        P_T_pre[:,j] = np.exp(-r*(T-tau)) * np.maximum(K-S_T[:,j],0)\r\n",
    "    P_T = np.sum(P_T_pre, axis=1)\r\n",
    "    return S_tau,P_T\r\n",
    "\r\n",
    "#the function DT(Z,\\theta)\r\n",
    "def data_trans_IS(Z,IS):\r\n",
    "    res = np.empty((len(Z),20))\r\n",
    "    for j in range(20):\r\n",
    "        res[:,j] = Z[:,j]*np.sqrt(IS[20+j]) + IS[j]\r\n",
    "    return res\r\n",
    "\r\n",
    "#The density function of Z\r\n",
    "def f(y):\r\n",
    "    return stats.multivariate_normal.pdf(y, mean=np.full(20,0), cov=np.identity(20))\r\n",
    "\r\n",
    "#The density function of Z_\\theta (note that x is interpreted as theta, needed for the least-squares solver to work properly)\r\n",
    "def f_theta(y, x):\r\n",
    "    return stats.multivariate_normal.pdf(y, mean=x[0:20], cov=np.diag(x[20:40]), allow_singular=True)\r\n",
    "\r\n",
    "#Put- und Call-Black-Scholes Formeln\r\n",
    "def d1(K, t, x, sigma, r):\r\n",
    "    return (np.log(x/K)+(r+0.5*sigma**2)*t)/(sigma*np.sqrt(t))\r\n",
    "\r\n",
    "def d2(K, t, x, sigma, r):\r\n",
    "    return (np.log(x/K)+(r-0.5*sigma**2)*t)/(sigma*np.sqrt(t))\r\n",
    "\r\n",
    "def put_true(K, t, x, sigma, r): \r\n",
    "    return K*np.exp(-r*t)*stats.norm.cdf(-d2(K,t,x,sigma,r)) - x*stats.norm.cdf(-d1(K,t,x,sigma,r))\r\n",
    "\r\n",
    "def call_true(K, t, x, sigma, r):\r\n",
    "    return x*stats.norm.cdf(d1(K,t,x,sigma,r)) - K*np.exp(-r*t)*stats.norm.cdf(d2(K,t,x,sigma,r))\r\n",
    "\r\n",
    "#true function l\r\n",
    "def P_T_true(x):\r\n",
    "    P_T = np.empty((len(x),20))\r\n",
    "    for j in range(0,10):\r\n",
    "        P_T[:,j] = call_true(K=100, t=(T-tau), x=x[:,j], sigma=sigma[j], r=r)\r\n",
    "        P_T[:,j+10] = put_true(K=100, t=(T-tau), x=x[:,j+10], sigma=sigma[j+10], r=r)\r\n",
    "    return np.sum(P_T, axis=1)\r\n",
    "\r\n",
    "#This function describes the approximation of the expression inside the sum of m_2(theta)\r\n",
    "def g_q_alpha_hat_reweighted(x,L,q_alpha_hat):\r\n",
    "    return np.sqrt(f(y=L[:,0:20])/f_theta(y=L[:,0:20],x=x))*(L[:,-1]>q_alpha_hat)\r\n",
    "\r\n",
    "#bounds for the IS density parameters (for the parameters corresponding to the mean no bounds are necessary, the standard deviation parameters however needs to be non-negative)\r\n",
    "bnds_lower = np.empty((40))\r\n",
    "bnds_upper = np.empty((40))\r\n",
    "for j in range(20):\r\n",
    "    bnds_lower[j] = -np.inf\r\n",
    "    bnds_upper[j] = np.inf\r\n",
    "    bnds_lower[20+j] = 0\r\n",
    "    bnds_upper[20+j] = np.inf\r\n",
    "    \r\n",
    "bnds = (bnds_lower, bnds_upper)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Generating realisations of multivariate standard normal random variables\r\n",
    "Z_IS = np.random.multivariate_normal(mean=np.full(20,0), cov=np.identity(20), size=M_IS)\r\n",
    "V_IS = np.random.multivariate_normal(mean=np.full(20,0), cov=np.identity(20), size=M_IS)\r\n",
    "\r\n",
    "#Calculate the risk factor S_tau and the corresponding simulated payoffs P_T\r\n",
    "S_tau_IS, P_T_IS = data_gen(M=M_IS, N=1, Z=Z_IS, V=V_IS)\r\n",
    "\r\n",
    "#Calculate realisations of L_hat from the training data set using the true function l\r\n",
    "L_hat_IS = np.column_stack((Z_IS, P_T_true(S_tau_IS)))\r\n",
    "L_hat_IS_sort = L_hat_IS[L_hat_IS[:,-1].argsort()[::-1]]\r\n",
    "\r\n",
    "#Calculating the corresponding estimator for Value-at-Risk in order to approximate g\r\n",
    "q_alpha_hat_IS = L_hat_IS_sort[int(M_IS*(1-alpha_IS)-1), -1]\r\n",
    "print('q_alpha_hat_IS:',q_alpha_hat_IS)\r\n",
    "\r\n",
    "#Calculating the (hopefully) approximately optimal \\theta^* by minimising m_2 using the approximated g\r\n",
    "IS = optimize.least_squares(g_q_alpha_hat_reweighted, x0=np.concatenate((np.full(20,0),np.full(20,1))), args=(L_hat_IS, q_alpha_hat_IS), bounds=bnds).x\r\n",
    "\r\n",
    "print(IS)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "q_alpha_hat_IS: 110.2277654560932\n",
      "[ 0.84624229  0.66549677  0.55264944  0.48526513  0.44432831  0.41710979\n",
      "  0.38894521  0.3727674   0.37558848  0.37692719 -0.58150006 -0.56273828\n",
      " -0.54571362 -0.51313112 -0.50300692 -0.50788231 -0.48373343 -0.48509131\n",
      " -0.48892469 -0.47385691  1.71577507  1.41780252  1.31571827  1.23343008\n",
      "  1.18449956  1.11683765  1.17822882  1.14104732  1.14259871  1.09038672\n",
      "  1.07997994  1.10912008  1.08283955  1.11709199  1.10135617  1.10486008\n",
      "  1.10710997  1.09278103  1.13279998  1.10300196]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "run = 1\r\n",
    "for j in range(100):\r\n",
    "    #Generating simulations for multivariate standard normal random variables for  Monte Carlo estimation of risk measures\r\n",
    "    Z_MC = np.random.multivariate_normal(mean=np.full(20,0), cov=np.identity(20), size=M_MC)\r\n",
    "    V_MC = np.random.multivariate_normal(mean=np.full(20,0), cov=np.identity(20), size=M_MC)\r\n",
    "    #calculating DT(Z,\\theta^*)\r\n",
    "    Z_MC_IS = data_trans_IS(Z_MC,IS)\r\n",
    "\r\n",
    "    #Calculate the risk factor S_tau and the corresponding simulated payoffs P_T with original and IS distribution\r\n",
    "    S_tau_MC,P_T_MC = data_gen(M=len(Z_MC),N=1,Z=Z_MC,V=V_MC)\r\n",
    "    S_tau_MC_IS,P_T_MC_IS = data_gen(M=len(Z_MC),N=1,Z=Z_MC_IS,V=V_MC)\r\n",
    "    \r\n",
    "    #computation of option price depending on the risk factor S_tau according to the models, i.e. computation of L_hat_i's with original distribution\r\n",
    "    L_hat = P_T_true(x=S_tau_MC)\r\n",
    "    L_hat_sort = np.sort(L_hat)[::-1]\r\n",
    "    \r\n",
    "    #calculation of Value-at-Risk and Expected Shortfall estimators without IS\r\n",
    "    j_VaR = int(M_MC*(1-alpha_VaR)-1)\r\n",
    "    VaR_hat = L_hat_sort[j_VaR]\r\n",
    "    \r\n",
    "    j_ES = int(M_MC*(1-alpha_ES)-1)\r\n",
    "    ES_hat = (1/(1-alpha_ES)) * np.sum(L_hat_sort[0:j_ES-1])/M_MC + ( 1 - (j_ES-1)/((1-alpha_ES)*M_MC) )*L_hat_sort[j_ES]\r\n",
    "    \r\n",
    "    #computation of option price depending on the risk factor S_tau according to the models, i.e. computation of L_hat_i's with IS distribution\r\n",
    "    L_hat_IS = P_T_true(x=S_tau_MC_IS)\r\n",
    "    L_hat_c_IS = np.column_stack((Z_MC_IS, L_hat_IS))\r\n",
    "    \r\n",
    "    #calculation of Value-at-Risk and Expected Shortfall estimators with IS\r\n",
    "    L_hat_c_sort_IS = L_hat_c_IS[L_hat_c_IS[:,-1].argsort()[::-1]]\r\n",
    "    w = f(L_hat_c_sort_IS[:,0:20])/(M_MC*f_theta(x=IS, y=L_hat_c_sort_IS[:,0:20]))\r\n",
    "\r\n",
    "    j_VaR = 0\r\n",
    "    w_sum_tmp = 0\r\n",
    "    while (w_sum_tmp <= (1-alpha_VaR) and j_VaR<M_MC):\r\n",
    "        w_sum_tmp += w[j_VaR]\r\n",
    "        j_VaR += 1\r\n",
    "    VaR_hat_IS = L_hat_c_sort_IS[j_VaR,-1]\r\n",
    "\r\n",
    "    j_ES = 0\r\n",
    "    w_sum_tmp = 0\r\n",
    "    while (w_sum_tmp <= (1-alpha_ES) and j_ES<M_MC):\r\n",
    "        w_sum_tmp += w[j_ES]\r\n",
    "        j_ES += 1\r\n",
    "    ES_hat_IS = (1/(1-alpha_ES)) * np.sum(w[0:j_ES-1]*L_hat_c_sort_IS[0:j_ES-1,-1]) + ( 1 - (1 / (1-alpha_ES)) * np.sum(w[0:j_ES-1]) )*L_hat_c_sort_IS[j_ES,-1]\r\n",
    "    \r\n",
    "    #save results for further evaluation\r\n",
    "    output = np.array([VaR_hat,VaR_hat_IS,ES_hat,ES_hat_IS])\r\n",
    "    \r\n",
    "    joblib.dump(output, filepath+'output'+str(j)+'_'+str(run)+'.joblib')\r\n",
    "    #prints just for checking while the notebook is running\r\n",
    "    print(j)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b88b057cf34efecd776605b6493619ffcb4d411e0887d16b1b551b502979a3a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}