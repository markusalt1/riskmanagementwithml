{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Results were produced in stints. This is the number of the last stint.\r\n",
    "run = 19"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "from sklearn.utils import shuffle\r\n",
    "from scipy import stats\r\n",
    "from scipy import optimize\r\n",
    "import joblib\r\n",
    "\r\n",
    "#folder for saving results\r\n",
    "filepath = \".../Resultate_final/PTF/IS/PTF_VaR_ES_IS_sim_saved/\""
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/slurm_tmpdir/job_20093333/ipykernel_1536966/3449009880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Bring in subpackages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# from tensorflow.python import keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/tensorflow/python/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/tensorflow/python/data/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_sparse_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/tensorflow/python/data/experimental/service/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_dataset_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_options\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExternalStatePolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimization_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtype_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSaverBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_io_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/tensorflow/python/ops/io_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TextLineReader\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTextLineReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReaderBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m   \"\"\"A Reader that outputs the lines of a file delimited by newlines.\n",
      "\u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2021-09-30/lib/python3.8/site-packages/tensorflow/python/util/tf_export.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m           \u001b[0mdifferent\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_names_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'v1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'v2'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Market and option parameters as in section 4.2 of 'Assessing Asset-Liability Risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "s_0 = 100\r\n",
    "r = 0.01\r\n",
    "corr= 0.3\r\n",
    "tau = 1/52\r\n",
    "T = 1/3\r\n",
    "K = 100\r\n",
    "\r\n",
    "mu = np.empty(20)\r\n",
    "sigma = np.empty(20)\r\n",
    "for i in range(0,10):\r\n",
    "    mu[i] = mu[i+10] = (3+(i+1)/2)/100\r\n",
    "    sigma[i] = sigma[i+10] = (15+(i+1))/100\r\n",
    "\r\n",
    "cov_mat = np.empty((20,20))\r\n",
    "for i in range(0,20):\r\n",
    "    for j in range(0,20):\r\n",
    "        if i != j:\r\n",
    "            cov_mat[i,j] = corr\r\n",
    "        else:\r\n",
    "            cov_mat[i,j] = 1\r\n",
    "\r\n",
    "C = np.linalg.cholesky(cov_mat)\r\n",
    "\r\n",
    "#Confidence levels and parameters for Value-at-Risk and Expected Shortfall\r\n",
    "alpha_VaR = 0.995\r\n",
    "alpha_ES = 0.99"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Sizes for training set, validation set, test set, and set size for Monte Carlo estimation of the risk measures\r\n",
    "M_1 = 1500000\r\n",
    "M_2 = 500000\r\n",
    "M_3 = 500000\r\n",
    "#ignore N or N_2 in the following. Was kept just in case, but not used.\r\n",
    "N_2 = 1\r\n",
    "M_MC = 500000\r\n",
    "#size of the set of data points used to calculate an IS density\r\n",
    "M_IS = 1000000\r\n",
    "#quantile for which the IS density will be computed\r\n",
    "alpha_IS = 0.995"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Function for calculating simulated values of S_tau and simulated payoffs P_T from simulations of multivariate standard normal random variables\r\n",
    "def data_gen(Z,V):\r\n",
    "    #correlating the independent components of Z\r\n",
    "    Z = np.transpose(np.matmul(C,np.transpose(Z)))\r\n",
    "    \r\n",
    "    #simulate S_tau under P\r\n",
    "    S_tau = np.empty((len(Z), 20))\r\n",
    "    for j in range(0,20):\r\n",
    "        S_tau[:,j] = s_0 * np.exp( (mu[j]-0.5*sigma[j]**2)*tau + np.sqrt(tau)*sigma[j]*Z[:,j] )\r\n",
    "\r\n",
    "    #simulate S_T given S_tau under Q\r\n",
    "    S_T = np.empty((len(Z),20))\r\n",
    "    for j in range(0,20):\r\n",
    "        S_T[:,j] = S_tau[:,j] * np.exp( (r-0.5*sigma[j]**2)*(T-tau) + np.sqrt(T-tau)*sigma[j]*V[:,j] )\r\n",
    "\r\n",
    "    #compute discounted option payoffs\r\n",
    "    P_T_pre =np.empty((len(S_T), 20))\r\n",
    "    for j in range(0,10):\r\n",
    "        P_T_pre[:,j] = np.exp(-r*(T-tau)) * np.maximum(S_T[:,j]-K,0)\r\n",
    "    for j in range(10,20):\r\n",
    "        P_T_pre[:,j] = np.exp(-r*(T-tau)) * np.maximum(K-S_T[:,j],0)\r\n",
    "    P_T = np.sum(P_T_pre, axis=1)\r\n",
    "    return S_tau,P_T\r\n",
    "\r\n",
    "#the function DT(Z,\\theta)\r\n",
    "def data_trans_IS(Z,IS):\r\n",
    "    res = np.empty((len(Z),20))\r\n",
    "    for j in range(20):\r\n",
    "        res[:,j] = Z[:,j]*np.sqrt(IS[20+j]) + IS[j]\r\n",
    "    return res\r\n",
    "\r\n",
    "#The density function of Z\r\n",
    "def f(y):\r\n",
    "    return stats.multivariate_normal.pdf(y, mean=np.full(20,0), cov=np.identity(20))\r\n",
    "\r\n",
    "#The density function of Z_\\theta (note that x is interpreted as theta, needed for the least-squares solver to work properly)\r\n",
    "def f_theta(y, x):\r\n",
    "    return stats.multivariate_normal.pdf(y, mean=x[0:20], cov=np.diag(x[20:40]), allow_singular=True)\r\n",
    "\r\n",
    "#This function describes the approximation of the expression inside the sum of m_2(theta)\r\n",
    "def g_q_alpha_hat_reweighted(x,L,q_alpha_hat):\r\n",
    "    return np.sqrt(f(y=L[:,0:20])/f_theta(y=L[:,0:20],x=x))*(L[:,-1]>q_alpha_hat)\r\n",
    "\r\n",
    "#bounds for the IS density parameters (for the parameters corresponding to the mean no bounds are necessary, the standard deviation parameters however needs to be non-negative)\r\n",
    "bnds_lower = np.empty((40))\r\n",
    "bnds_upper = np.empty((40))\r\n",
    "for j in range(20):\r\n",
    "    bnds_lower[j] = -np.inf\r\n",
    "    bnds_upper[j] = np.inf\r\n",
    "    bnds_lower[20+j] = 0\r\n",
    "    bnds_upper[20+j] = np.inf\r\n",
    "    \r\n",
    "bnds = (bnds_lower, bnds_upper)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for j in range(84):\r\n",
    "    #Generating realisations of multivariate standard normal random variables (uncorrelated for Z such that calculation of \\theta^* is possible and correlated for V)\r\n",
    "    Z_IS = np.random.multivariate_normal(mean=np.full(20,0), cov=np.identity(20), size=M_IS)\r\n",
    "    V_IS = np.random.multivariate_normal(mean=np.full(20,0), cov=cov_mat, size=M_IS)\r\n",
    "\r\n",
    "    #Calculate the risk factor S_tau and the corresponding simulated payoffs P_T\r\n",
    "    S_tau_IS, P_T_IS = data_gen(Z=Z_IS, V=V_IS)\r\n",
    "\r\n",
    "    #define and compile neural network model, setup as in section 4.2 of 'Assessing Asset-Liability Risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "    bi_IS = np.log( np.sum(P_T_IS)/len(P_T_IS))\r\n",
    "    model_IS = tf.keras.models.Sequential([\r\n",
    "        tf.keras.layers.BatchNormalization(input_shape=(20,)),\r\n",
    "        tf.keras.layers.Dense(20, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(20, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(1, activation='exponential', bias_initializer=tf.keras.initializers.Constant(value=bi_IS))])\r\n",
    "    model_IS.compile(loss='mse', optimizer='adam', metrics=['mse'])\r\n",
    "    model_IS.fit(x=S_tau_IS, y=P_T_IS, epochs=100, batch_size=10000, verbose=0)\r\n",
    "\r\n",
    "    #Calculate realisations of L_hat from the training data set using the trained neural network\r\n",
    "    L_hat_IS = np.column_stack((Z_IS, model_IS.predict(S_tau_IS)[:,0]))\r\n",
    "    L_hat_IS_sort = L_hat_IS[L_hat_IS[:,-1].argsort()[::-1]]\r\n",
    "\r\n",
    "    #Calculating the corresponding estimator for Value-at-Risk in order to approximate g\r\n",
    "    q_alpha_IS_hat = L_hat_IS_sort[int(M_IS*(1-alpha_IS)-1), -1]\r\n",
    "    print('q_alpha_IS_hat_NN:',q_alpha_IS_hat)\r\n",
    "\r\n",
    "    #Calculating the (hopefully) approximately \\theta^*_{NN} by minimising m_2 using the approximated g\r\n",
    "    IS_NN = optimize.least_squares(g_q_alpha_hat_reweighted, x0=np.concatenate((np.full(20,0),np.full(20,1))), args=(L_hat_IS, q_alpha_IS_hat), bounds=bnds).x\r\n",
    "\r\n",
    "    #define and train a random forest according to the optimal parameters from tuning\r\n",
    "    rfr_IS = RandomForestRegressor(n_estimators=160, criterion='squared_error', max_features=8, min_samples_leaf=70, bootstrap=True, verbose=0, n_jobs=-1)\r\n",
    "    rfr_IS.fit(X=S_tau_IS, y=P_T_IS)\r\n",
    "\r\n",
    "    #Calculate realisations of L_hat from the training data set using the trained random forest\r\n",
    "    L_hat_IS = np.column_stack((Z_IS, rfr_IS.predict(S_tau_IS)))\r\n",
    "    L_hat_IS_sort = L_hat_IS[L_hat_IS[:,-1].argsort()[::-1]]\r\n",
    "\r\n",
    "    #Calculating the corresponding estimator for Value-at-Risk in order to approximate g\r\n",
    "    q_alpha_IS_hat = L_hat_IS_sort[int(M_IS*(1-alpha_IS)-1), -1]\r\n",
    "    print('q_alpha_IS_hat_RF:',q_alpha_IS_hat)\r\n",
    "\r\n",
    "    #Calculating the (hopefully) approximately optimal \\theta^*_{RF} by minimising m_2 using the approximated g\r\n",
    "    IS_RF = optimize.least_squares(g_q_alpha_hat_reweighted, x0=np.concatenate((np.full(20,0),np.full(20,1))), args=(L_hat_IS, q_alpha_IS_hat),bounds=bnds).x\r\n",
    "\r\n",
    "    #print IS density parameters for checking\r\n",
    "    print('IS_NN:',IS_NN)\r\n",
    "    print('IS_RF:',IS_RF)\r\n",
    "    \r\n",
    "    #Generating simulations for multivariate standard normal random variables (uncorrelated for Z, correlated for V) for training set, validation set, test set, set for Monte Carlo estimation of risk measures\r\n",
    "    Z_train = np.random.multivariate_normal(mean=np.full(20,0), cov=np.identity(20), size=M_1)\r\n",
    "    V_train = np.random.multivariate_normal(mean=np.full(20,0), cov=cov_mat, size=M_1)\r\n",
    "    Z_val = np.random.multivariate_normal(mean=np.full(20,0), cov=np.identity(20), size=M_2)\r\n",
    "    V_val = np.random.multivariate_normal(mean=np.full(20,0), cov=cov_mat, size=M_2)\r\n",
    "    Z_test = np.random.multivariate_normal(mean=np.full(20,0), cov=np.identity(20), size=M_3)\r\n",
    "    V_test = np.random.multivariate_normal(mean=np.full(20,0), cov=cov_mat, size=M_3)\r\n",
    "    Z_MC = np.random.multivariate_normal(mean=np.full(20,0), cov=np.identity(20), size=M_MC)\r\n",
    "    V_MC = np.random.multivariate_normal(mean=np.full(20,0), cov=cov_mat, size=M_MC)\r\n",
    "\r\n",
    "    #calculating DT(Z,\\theta^*_{NN})\r\n",
    "    Z_train_NN = data_trans_IS(Z_train,IS_NN)\r\n",
    "    Z_val_NN = data_trans_IS(Z_val,IS_NN)\r\n",
    "    Z_test_NN = data_trans_IS(Z_test,IS_NN)\r\n",
    "    Z_MC_NN = data_trans_IS(Z_MC,IS_NN)\r\n",
    "    #calculating the risk factors under the IS distribution and corresponding option prices\r\n",
    "    S_tau_train_NN, P_T_train_NN = data_gen(Z=Z_train_NN, V=V_train)\r\n",
    "    S_tau_val_NN, P_T_val_NN = data_gen(Z=Z_val_NN, V=V_val)\r\n",
    "    S_tau_test_NN, P_T_test_NN = data_gen(Z=Z_test_NN, V=V_test)\r\n",
    "    S_tau_MC_NN, P_T_MC_NN = data_gen(Z=Z_MC_NN, V=V_MC)\r\n",
    "\r\n",
    "    #calculating DT(Z,\\theta^*_{RF})\r\n",
    "    Z_train_RF = data_trans_IS(Z_train,IS_RF)\r\n",
    "    Z_val_RF = data_trans_IS(Z_val,IS_RF)\r\n",
    "    Z_test_RF = data_trans_IS(Z_test,IS_RF)\r\n",
    "    Z_MC_RF = data_trans_IS(Z_MC,IS_RF)\r\n",
    "    #calculating the risk factors under the IS distribution and corresponding option prices\r\n",
    "    S_tau_train_RF, P_T_train_RF = data_gen(Z=Z_train_RF, V=V_train)\r\n",
    "    S_tau_val_RF, P_T_val_RF = data_gen(Z=Z_val_RF, V=V_val)\r\n",
    "    S_tau_test_RF, P_T_test_RF = data_gen(Z=Z_test_RF, V=V_test)\r\n",
    "    S_tau_MC_RF, P_T_MC_RF = data_gen(Z=Z_MC_RF, V=V_MC)\r\n",
    "    \r\n",
    "    #calculating parameters for the sets B_1 and B_2\r\n",
    "    s_20_1 = s_0 * np.exp((mu[0:3]-0.5*sigma[0:3]**2)*tau + sigma[0:3]*np.sqrt(tau)*stats.norm.ppf(0.2, loc=0, scale=1))\r\n",
    "    s_80_1 = s_0 * np.exp((mu[9:12]-0.5*sigma[9:12]**2)*tau + sigma[9:12]*np.sqrt(tau)*stats.norm.ppf(0.8, loc=0, scale=1))\r\n",
    "    s_20_2 = s_0 * np.exp((mu[9:12]-0.5*sigma[9:12]**2)*tau + sigma[9:12]*np.sqrt(tau)*stats.norm.ppf(0.2, loc=0, scale=1))\r\n",
    "    s_80_2 = s_0 * np.exp((mu[0:3]-0.5*sigma[0:3]**2)*tau + sigma[0:3]*np.sqrt(tau)*stats.norm.ppf(0.8, loc=0, scale=1))\r\n",
    "\r\n",
    "    #calculate the indices of the set B_1 and B_2 for the test set created with the IS density calculated by the neural network\r\n",
    "    B_1_NN = np.apply_along_axis(np.all, axis=1, arr=np.column_stack( (np.apply_along_axis(np.all, axis=1, arr=S_tau_test_NN[:,0:3] > s_20_1), np.apply_along_axis(np.all, axis=1, arr=S_tau_test_NN[:,9:12] < s_80_1)) ) )\r\n",
    "    B_2_NN = np.apply_along_axis(np.all, axis=1, arr=np.column_stack( (np.apply_along_axis(np.all, axis=1, arr=S_tau_test_NN[:,0:3] < s_80_2), np.apply_along_axis(np.all, axis=1, arr=S_tau_test_NN[:,9:12] > s_20_2)) ) )\r\n",
    "\r\n",
    "    #calculate the indices of the set B_1 and B_2 for the test set created with the IS density calculated by the random forest\r\n",
    "    B_1_RF = np.apply_along_axis(np.all, axis=1, arr=np.column_stack( (np.apply_along_axis(np.all, axis=1, arr=S_tau_test_RF[:,0:3] > s_20_1), np.apply_along_axis(np.all, axis=1, arr=S_tau_test_RF[:,9:12] < s_80_1)) ) )\r\n",
    "    B_2_RF = np.apply_along_axis(np.all, axis=1, arr=np.column_stack( (np.apply_along_axis(np.all, axis=1, arr=S_tau_test_RF[:,0:3] < s_80_2), np.apply_along_axis(np.all, axis=1, arr=S_tau_test_RF[:,9:12] > s_20_2)) ) )\r\n",
    "    \r\n",
    "    #define and compile neural network model, setup as in section 4.2 of 'Assessing Asset-Liability Risk with Neural Networks' (Cheridito, Ery, Wüthrich 2020)\r\n",
    "    bi = np.log( np.sum(P_T_train_NN)/len(P_T_train_NN))\r\n",
    "    model = tf.keras.models.Sequential([\r\n",
    "        tf.keras.layers.BatchNormalization(input_shape=(20,)),\r\n",
    "        tf.keras.layers.Dense(20, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(20, activation='tanh'),\r\n",
    "        tf.keras.layers.Dense(1, activation='exponential', bias_initializer=tf.keras.initializers.Constant(value=bi))])\r\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\r\n",
    "    #training the neural network\r\n",
    "    hist = model.fit(x=S_tau_train_NN, y=P_T_train_NN, epochs=100, batch_size=10000, validation_data=(S_tau_val_NN,P_T_val_NN), verbose=0)\r\n",
    "    \r\n",
    "    #computation of the metrics (a), (b), (c) with B_1 and (c) with B_2 for the neural network\r\n",
    "    P_T_pred_NN = model.predict(S_tau_test_NN)[:,0]\r\n",
    "    mse_train_NN = hist.history['mse'][-1]\r\n",
    "    mse_val_NN = hist.history['val_mse'][-1]\r\n",
    "    mc_tmp = P_T_pred_NN - P_T_test_NN\r\n",
    "    metric_a_NN = np.sum(mc_tmp)/len(P_T_test_NN)\r\n",
    "    metric_b_NN = np.sum((mc_tmp)*P_T_pred_NN)/len(P_T_test_NN)\r\n",
    "    metric_c_B_1_NN = np.sum(mc_tmp[B_1_NN])/len(P_T_test_NN)\r\n",
    "    metric_c_B_2_NN = np.sum(mc_tmp[B_2_NN])/len(P_T_test_NN)\r\n",
    "\r\n",
    "    #computation of option price depending on the risk factor S_tau according to the models, i.e. computation of L_hat_i's\r\n",
    "    L_hat_NN = model.predict(S_tau_MC_NN)[:,0]\r\n",
    "    L_hat_c_NN = np.column_stack((Z_MC_NN, L_hat_NN))\r\n",
    "    \r\n",
    "    #calculation of the IS estimator for Value-at-Risk and Expected Shortfall\r\n",
    "    L_hat_c_sort_NN = L_hat_c_NN[L_hat_c_NN[:,-1].argsort()[::-1]]\r\n",
    "    w = f(L_hat_c_sort_NN[:,0:20])/(M_MC*f_theta(x=IS_NN, y=L_hat_c_sort_NN[:,0:20]))\r\n",
    "\r\n",
    "    j_VaR = 0\r\n",
    "    w_sum_tmp = 0\r\n",
    "    while (w_sum_tmp <= (1-alpha_VaR) and j_VaR<M_MC):\r\n",
    "        w_sum_tmp += w[j_VaR]\r\n",
    "        j_VaR += 1\r\n",
    "    VaR_hat_NN = L_hat_c_sort_NN[j_VaR,-1]\r\n",
    "    print('VaR_hat_NN:',VaR_hat_NN)\r\n",
    "    \r\n",
    "    j_ES = 0\r\n",
    "    w_sum_tmp = 0\r\n",
    "    while (w_sum_tmp <= (1-alpha_ES) and j_ES<M_MC):\r\n",
    "        w_sum_tmp += w[j_ES]\r\n",
    "        j_ES += 1\r\n",
    "    ES_hat_NN = (1/(1-alpha_ES)) * np.sum(w[0:j_ES-1]*L_hat_c_sort_NN[0:j_ES-1,-1]) + ( 1 - (1 / (1-alpha_ES)) * np.sum(w[0:j_ES-1]) )*L_hat_c_sort_NN[j_ES,-1]\r\n",
    "    print('ES_hat_NN:',ES_hat_NN)\r\n",
    "    \r\n",
    "    \r\n",
    "    #perform a grid search in order to find the (approximately) best hyperparameter min_samples_leaf\r\n",
    "    #values that will be checked\r\n",
    "    max_features_list = [8]\r\n",
    "    min_samples_leaf_list = [30,40,50]\r\n",
    "    opt_param = np.full(2,0)\r\n",
    "    opt_score = np.inf\r\n",
    "\r\n",
    "    for max_features in max_features_list:\r\n",
    "        for min_samples_leaf in min_samples_leaf_list:\r\n",
    "            rfr_tuning = RandomForestRegressor(n_estimators=160, max_features=max_features, min_samples_leaf=min_samples_leaf, bootstrap=True, criterion='squared_error', verbose=0, n_jobs=-1)\r\n",
    "            rfr_tuning.fit(X=S_tau_train_RF, y=P_T_train_RF)\r\n",
    "            score = mean_squared_error(y_true=P_T_val_RF, y_pred=rfr_tuning.predict(S_tau_val_RF))\r\n",
    "            if score < opt_score:\r\n",
    "                opt_param_RF = np.array([max_features,min_samples_leaf])\r\n",
    "                opt_score = score\r\n",
    "    \r\n",
    "    #definition and training of random forest regressor\r\n",
    "    rfr = RandomForestRegressor(n_estimators=400, criterion='squared_error', max_features=int(opt_param_RF[0]), min_samples_leaf=int(opt_param_RF[1]), bootstrap=True, verbose=0, warm_start=True, n_jobs=-1)\r\n",
    "    rfr.fit(X=S_tau_train_RF, y=P_T_train_RF)\r\n",
    "    \r\n",
    "    #computation of the metrics (a), (b), (c) with B_1 and (c) with B_2 and training/valdiation MSE for the random forest\r\n",
    "    mse_train_RF = mean_squared_error(y_true=P_T_train_RF, y_pred=rfr.predict(S_tau_train_RF))\r\n",
    "    mse_val_RF = mean_squared_error(y_true=P_T_val_RF, y_pred=rfr.predict(S_tau_val_RF))\r\n",
    "    P_T_pred_RF = rfr.predict(S_tau_test_RF)\r\n",
    "    mc_tmp = P_T_pred_RF - P_T_test_RF\r\n",
    "    metric_a_RF = np.sum(mc_tmp)/len(P_T_test_RF)\r\n",
    "    metric_b_RF = np.sum((mc_tmp)*P_T_pred_RF)/len(P_T_test_RF)\r\n",
    "    metric_c_B_1_RF = np.sum(mc_tmp[B_1_RF])/len(P_T_test_RF)\r\n",
    "    metric_c_B_2_RF = np.sum(mc_tmp[B_2_RF])/len(P_T_test_RF)\r\n",
    "\r\n",
    "    #computation of option price depending on the risk factor S_tau according to the models, i.e. computation of L_hat_i's\r\n",
    "    L_hat_RF = rfr.predict(S_tau_MC_RF)\r\n",
    "    L_hat_c_RF = np.column_stack((Z_MC_RF, L_hat_RF))\r\n",
    "    \r\n",
    "    #calculation of the IS estimators for Value-at-Risk and Expected Shortfall\r\n",
    "    L_hat_c_sort_RF = L_hat_c_RF[L_hat_c_RF[:,-1].argsort()[::-1]]\r\n",
    "    w = f(L_hat_c_sort_RF[:,0:20])/(M_MC*f_theta(x=IS_RF, y=L_hat_c_sort_RF[:,0:20]))\r\n",
    "\r\n",
    "    j_VaR = 0\r\n",
    "    w_sum_tmp = 0\r\n",
    "    while (w_sum_tmp <= (1-alpha_VaR) and j_VaR<M_MC):\r\n",
    "        w_sum_tmp += w[j_VaR]\r\n",
    "        j_VaR += 1\r\n",
    "    VaR_hat_RF = L_hat_c_sort_RF[j_VaR,-1]\r\n",
    "    print('VaR_hat_RF:',VaR_hat_RF)\r\n",
    "    \r\n",
    "    j_ES = 0\r\n",
    "    w_sum_tmp = 0\r\n",
    "    while (w_sum_tmp <= (1-alpha_ES) and j_ES<M_MC):\r\n",
    "        w_sum_tmp += w[j_ES]\r\n",
    "        j_ES += 1\r\n",
    "    ES_hat_RF = (1/(1-alpha_ES)) * np.sum(w[0:j_ES-1]*L_hat_c_sort_RF[0:j_ES-1,-1]) + ( 1 - (1 / (1-alpha_ES)) * np.sum(w[0:j_ES-1]) )*L_hat_c_sort_RF[j_ES,-1]\r\n",
    "    print('ES_hat_RF:',ES_hat_RF)\r\n",
    "    \r\n",
    "    #save results for further evaluation\r\n",
    "    output = np.array([[mse_train_NN,mse_val_NN,metric_a_NN,metric_b_NN,metric_c_B_1_NN,metric_c_B_2_NN,VaR_hat_NN,ES_hat_NN],\r\n",
    "                      [mse_train_RF,mse_val_RF,metric_a_RF,metric_b_RF,metric_c_B_1_RF,metric_c_B_2_RF,VaR_hat_RF,ES_hat_RF]])\r\n",
    "\r\n",
    "    joblib.dump(output,filepath+'output_'+str(run)+'_'+str(j)+'.joblib')\r\n",
    "    #since IS_NN and IS_RF are so high-dimensional we save them separately in order to avoid confusion in the later evaluation of results\r\n",
    "    joblib.dump(IS_NN, '.../Resultate_final/PTF/IS/PTF_VaR_ES_IS_sim_NN_densities_saved/'+'output_'+str(run)+'_'+str(j)+'.joblib')\r\n",
    "    joblib.dump(IS_RF, '.../Resultate_final/PTF/IS/PTF_VaR_ES_IS_sim_RF_densities_saved/'+'output_'+str(run)+'_'+str(j)+'.joblib')\r\n",
    "    #prints just for checking while the notebook is running\r\n",
    "    print(j)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}